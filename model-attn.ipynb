{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Credits:\n- https://www.kaggle.com/code/jhjh97/sleep-critical-point-train/edit\n- https://www.kaggle.com/code/jhjh97/detect-sleep-states-train/edit\n\n## PATHS  \n- Data Preparation: https://www.kaggle.com/code/jhjh97/transformer-data-preprocessing  \n- TRAIN - this  \n- INFERENCE - not yet","metadata":{"_uuid":"4d5d768a-dae7-4969-9c07-6ac634eb8913","_cell_guid":"3aa06935-4451-4f72-879f-8c5dbf211eaf","trusted":true}},{"cell_type":"markdown","source":"# Import and Config","metadata":{"_uuid":"e3ad3339-a3d4-4d3c-ad8c-ac67a38fc9a3","_cell_guid":"46e7aa50-af91-423c-931e-0ffe80386438","trusted":true}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nimport os, glob\nimport random\nimport math\n\nfrom tqdm.auto import tqdm \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nfrom sklearn.metrics import average_precision_score\nfrom timm.scheduler import CosineLRScheduler\n\nfrom collections import OrderedDict\nfrom transformers import get_cosine_schedule_with_warmup\nimport time\nfrom metric import event_detection_ap","metadata":{"_uuid":"10952817-cc05-4018-9661-c1a1b030dfff","_cell_guid":"21231572-2e25-4a0b-b572-e87501c78c3a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-09T13:16:09.425374Z","iopub.execute_input":"2023-11-09T13:16:09.425730Z","iopub.status.idle":"2023-11-09T13:16:22.517311Z","shell.execute_reply.started":"2023-11-09T13:16:09.425700Z","shell.execute_reply":"2023-11-09T13:16:22.516351Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"class CONFIG:\n    # PATHS\n    INPUT_DIR = '/kaggle/input/sample'\n    \n    # Fundamental config\n    NOTDEBUG = True # False -> DEBUG, True -> normally train\n    WORKERS = os.cpu_count()-1\n    N_FOLDS = 5\n    TRAIN_FOLD = 0\n    MAX_LEN = 2**14\n    USE_AMP = True\n    SEED = 42\n    \n    # Data config\n    ROLLING_RANGES = [17, 33, 65]\n    N_DAYS_PER_SAMPLE = 3 # same with data prep config\n    SHUFFLE=True\n    FOLDS=5\n    \n    # Model config\n    IN_CHANNEL = 1 + len(ROLLING_RANGES)*2\n    SEQ_LEN = 24*60*12*N_DAYS_PER_SAMPLE\n    D_MODEL = 64 if NOTDEBUG else 16\n#     KERNEL_SIZE = 30\n    N_BLKS = 1 if NOTDEBUG else 1\n    DROPOUT = 0.2\n    \n    # Optimizer config\n    LR = 5e-4\n    WD = 1e-2\n    WARMUP_PROP = 0.1\n    # LR_INIT = 1e-4\n    # LR_MIN = 1e-5\n    \n    # Train config\n    EPOCHS = 20\n    BS = 4\n#     MAX_GRAD_NORM = 2.\n#     GRAD_ACC = 32 // BS","metadata":{"_uuid":"06fe6af4-2e6a-4eaf-a3bf-83f28a480909","_cell_guid":"065c6757-df14-4f73-a42b-fc86be4a7914","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-09T13:16:22.519140Z","iopub.execute_input":"2023-11-09T13:16:22.520176Z","iopub.status.idle":"2023-11-09T13:16:22.527104Z","shell.execute_reply.started":"2023-11-09T13:16:22.520138Z","shell.execute_reply":"2023-11-09T13:16:22.526103Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def torch_fix_seed(seed=42):\n    # Python random\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n    # Pytorch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # torch.backends.cudnn.deterministic = True\n    # torch.use_deterministic_algorithms = True\n    # torch.backends.cudnn.benchmark = True\n\ntorch_fix_seed(CONFIG.SEED)\ntorch.set_default_dtype(torch.float32)","metadata":{"_uuid":"f0ae7139-e7b6-4516-9b2f-2213e786f3be","_cell_guid":"bde2a914-21fb-41bf-b780-6dc0621432ec","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-09T13:16:22.528266Z","iopub.execute_input":"2023-11-09T13:16:22.528587Z","iopub.status.idle":"2023-11-09T13:16:22.543068Z","shell.execute_reply.started":"2023-11-09T13:16:22.528558Z","shell.execute_reply":"2023-11-09T13:16:22.542325Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"_uuid":"e64d0e62-1749-43b6-aead-e0d3fe88d96b","_cell_guid":"22a0b866-02b2-4d3b-aa44-b175e2a4b000","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-09T13:16:22.545041Z","iopub.execute_input":"2023-11-09T13:16:22.545330Z","iopub.status.idle":"2023-11-09T13:16:22.575924Z","shell.execute_reply.started":"2023-11-09T13:16:22.545307Z","shell.execute_reply":"2023-11-09T13:16:22.575128Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model","metadata":{"_uuid":"27a02a80-4cb3-49d6-8e34-c7fd97c49985","_cell_guid":"b7c1e5e2-6f1d-48b7-b2b8-d7f7a439b7b6","trusted":true}},{"cell_type":"markdown","source":"## Model Version1 : Encoder Only\n### 모델 의도 설명\n- 이미 attention에서 주변 맥락 압축해 현재 seq에 존재해야할 feature을 추렸어.\n- 주변과 전체 맥락을 보고 한번 좀 정리를 한거지.\n- 그리고 바로 한번 output 뽑아볼려고. decoder 없어도 처음에 의도했던 (주변 맥락을 deeplearning 적으로 추리기 + transformer)요소를 다 포함한 것 같아서.\n\n### Hmm..\n- attention이 너무 멀리보는것 같다는 우려가 난 있어 3일치 데이터니까, 1일차의 데이터가 3일차의 데이터와 연관도는 전혀 없을거거든. 이 부분 더 구체화 해보자.\n     - 데이터를 1일치로 낮춰봐도 좋을 듯?\n         - 근데 이러면 밤 12시 30분에 잔 애기는 앞에 30분의 데이터만으로 onset을 판단하도록 모델이 학습됨. 이건 모델에게서 더 멀리볼 수 있을 여지를 방해하는것같음. 얼마나 전 데이터까지가 연관있는지 모르니깐,,, 일단 전날 훗날 6시간씩 줘보면 적당하지 않을까?\n     - 혹은 데이터를 1일치 + 8시간? 으로 낮춰봐도 좋을듯?  \n     - **결론적으로! 요 세개에 대해 실험 해보자!**\n         1. 데이터 앞뒤+1일(총 3일)\n         2. 데이터 1일 (진짜 성능 떨어짐?)\n         3. 데이터 1일+6시간 (제일 적절할거라 예상)","metadata":{"_uuid":"ddc1c817-ef3a-425f-afbd-c514763aceca","_cell_guid":"6c16bb5f-91b4-4d17-bfb6-522e551e06bd","trusted":true}},{"cell_type":"markdown","source":"### Aggregator\n\n- 목적은? \n    - N steps만큼을 요약해서 요약된 것들끼리 attention을 구하는거야.\n    - 실제로 5초 단위의 데이터가 의미있을 것 같진 않고, 여러 step들이 묶여서 어떤 패턴을 가질 때 의미가 있을 것 같거든\n    - 그래서 1분, 3분, 5분, ... 단위로 Aggregation을 시켜서 이 뭉탱이들 끼리의 attention을 구하도록 유도하는게 이 Aggregator의 목적이야.\n    - 제일 적절한 aggregation 범위는 실험으로 얻어야 할 것 같구.  \n\n\n- 방법은?\n    - 핵심은 주어진 scale만큼으로 seq를 요약해주는 것이야.\n    - 어떤 pooling 방식이든, stride가 aggregate_scale과 일치하면 괜찮을 것 같아.\n    - AvgPooling으로 특정 stride써서 얻을 수 도 있고, CNN으로 지금 sacle보다 더 넓게 보면서 구해도 좋을 것 같고.\n        - 5초 단위의 변화가 중요한 요소일 수 도 있는데, pooling이 적절할지 모르겠네.\n    - 가장 적절한 방식은 실험으로 얻어야겠지만, 어떤 scale범위를 대표하는 features를 만들기위해 더 넓게 본다면 (지금 task에서 event가 상당히 sparse하기 때문에)탐색범위가 넓어져서 훨씬 좋을 것 같네.\n    - 그래서 예상으론 일반 Pooling 방식들보단 CNN같은 더 넓게 보는 방식이 더 좋은 결과를 낼 것 같아.\n    \n\n- **IDEAS TO BE TESTED**\n    1. CNN 한번으로 많이 요약하지 말고, 짧은 CNN 여러개로 계층적으로 요약하기(ResNet 방식처럼)\n        1. 직렬적\n        2. 병렬적\n        - 1/12, 1/12, 1/4 요약할 예정.\n    2. 여러 seq에 대한 attention 사용하기.\n        - Seq를 1시간 단위로 압축한, 30분단위로 압축한 5분 단위로 압축된 attn (Seq d) 들 구해서, 걔들을 sum해서 사용하는건 어떨까?\n    3. 다양한 kernel_size/stride 테스트(초반)","metadata":{"_uuid":"b38c1549-2137-4c94-8708-d06d93889482","_cell_guid":"190f518e-ace0-4c66-9ff8-555b88a672a2","trusted":true}},{"cell_type":"code","source":"# class Aggregator(nn.Module):\n#     # Experiments needed\n    \n#     def __init__(self, d_model, strategy, agg_scale=12):\n#         super().__init__()\n#         if strategy=='avg':\n#             self.pool = nn.AvgPool1d(kernel_size=agg_scale, stride=agg_scale, ceil_mode=True)\n#         elif strategy=='max':\n#             self.pool = nn.MaxPool1d(kernel_size=agg_scale, stride=agg_scale, ceil_mode=True)\n#         elif strategy=='cnn1':\n#             self.pool = nn.Sequential(\n#                 nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=10, stride=6, padding=4), nn.ReLU(),\n#                 nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=10, stride=6, padding=4), nn.ReLU(),\n#             )\n#         elif strategy=='cnn2':\n#             self.pool = nn.Sequential(\n#                 nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=10, stride=6, padding=4), nn.ReLU(),\n#                 nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=10, stride=6, padding=4), nn.ReLU(),\n#                 nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=10, stride=6, padding=4), nn.ReLU(),\n#             )\n#         elif strategy=='cnn3':\n#             self.pool = nn.Sequential(\n#                 nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=10, stride=6, padding=4), nn.ReLU(),\n#                 nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=10, stride=6, padding=4), nn.ReLU(),\n#                 nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=10, stride=6, padding=4), nn.ReLU(),\n#                 nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=10, stride=5, padding=5), nn.ReLU(),\n\n#             )\n#         else:\n#             assert False, \"Choose proper strategy among: 'max', 'avg', 'cnn1,2,3'\"\n    \n#     def forward(self, x):\n#         x = torch.permute(x, (0,2,1))\n#         x = self.pool(x)\n#         x = torch.permute(x, (0,2,1))\n#         return x","metadata":{"_uuid":"398b8c5e-ff2c-4f27-afbf-703b58f05355","_cell_guid":"55c08414-4222-4b81-a359-ea9ee5f141ff","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Self Attention\n# class TimeAttention(nn.Module):\n    \n#     def __init__(self, d_model, pooling_strategy, dropout=0.1, agg_scale=12):\n#         super().__init__()\n#         self.scale = d_model ** 0.5\n#         self.strategy = pooling_strategy\n#         self.agg_sacle = agg_scale\n#         self.aggregate_q = Aggregator(d_model, pooling_strategy, agg_scale)  # 37056\n#         self.aggregate_v = Aggregator(d_model, pooling_strategy, agg_scale)  # 37056\n#         self.w_k = nn.Linear(d_model, d_model, bias = False)  # params= 4096\n        \n#         self.attn_dropout = nn.Dropout(dropout)\n#         self.dropout = nn.Dropout(dropout)\n#         self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n        \n#     def forward(self, q, k, v, mask=None):\n#         residual = k.clone()\n        \n#         q = self.aggregate_q(q)\n#         k = self.w_k(k)\n#         v = self.aggregate_v(v)\n        \n#         attn = torch.matmul(k / self.scale, q.transpose(-1,-2))  # (seq d)*(d seq/12) = (seq, seq/12)\n#         torch.cuda.memory_summary()\n        \n#         if mask is not None:\n#             attn = attn.masked_fill(mask==0, -1e9)\n#         attn = self.attn_dropout(F.softmax(attn, dim= -1))  # (seq *seq/12)\n#         x = torch.matmul(attn, v)  # (seq *seq/12)*(seq/12 d)\n#         x = self.dropout(x)\n#         x += residual\n#         out = self.layer_norm(x)\n        \n#         # returning q,v might be potential problem.\n#         return out","metadata":{"_uuid":"6f59344d-4432-4a5a-864b-2b1d182a42a4","_cell_guid":"8334dbe0-2d52-42ef-98fc-d0251a682a21","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class FeedForwardNN(nn.Module):\n    \n#     def __init__(self, d_model, d_hid, dropout=0.1):\n#         super().__init__()\n#         self.w_1 = nn.Linear(d_model, d_hid)  # params= 16384\n#         self.w_2 = nn.Linear(d_hid, d_model)  # params= 16384\n#         self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n#         self.dropout = nn.Dropout(dropout)\n\n        \n#     def forward(self, x):\n#         residual = x.clone()\n#         x = self.w_2(F.relu(self.w_1(x)))\n#         x = self.dropout(x)\n#         x += residual\n#         out = self.layer_norm(x)\n        \n#         return out","metadata":{"_uuid":"b22b6b4a-83a3-4d75-aab6-ccaa09760fd4","_cell_guid":"33019757-1b4b-4fce-9623-2d69a2f95b4d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Serial Architecture\n# class EncoderLayer(nn.Module):\n    \n#     def __init__(self,d_model, d_hid, dropout=0.1, agg_scale=12):\n#         super().__init__()\n#         self.attention1 = TimeAttention(d_model, 'cnn1', dropout, agg_scale)\n#         self.attention2 = TimeAttention(d_model, 'cnn2', dropout, agg_scale)\n#         self.attention3 = TimeAttention(d_model, 'cnn3', dropout, agg_scale)\n#         self.ffnn = FeedForwardNN(d_model, d_hid, dropout)\n        \n#     def forward(self, x):\n#         # attn*3, ffnn\n#         x = self.attention1(x, x, x)\n#         x = self.attention2(x, x, x)\n#         x = self.attention3(x, x, x)\n        \n#         x = self.ffnn(x)\n        \n#         return x","metadata":{"_uuid":"11b459cc-61b1-407b-bb73-916282c48b6b","_cell_guid":"7aedd4e9-4c5d-4b81-805e-76bcd95c45e9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class Encoder(nn.Module):\n    \n#     def __init__(self, n_blocks, in_channels, out_classes, d_model, d_hid, dropout=0.1, agg_scale=12):\n#         super(Encoder, self).__init__()\n#         self.fc_input = nn.Linear(in_channels, d_model, bias=True)  # bias true?\n#         self.encoder_blks = nn.ModuleList([EncoderLayer(d_model, d_hid, dropout, agg_scale) for _ in range(n_blocks)])\n#         self.fc_output = nn.Linear(d_model, out_classes)\n        \n#     def forward(self, x):\n        \n#         x = self.fc_input(x) # (B Seq d)\n#         for idx, encoder in enumerate(self.encoder_blks):\n#             x = encoder(x)\n#         x = self.fc_output(x)\n        \n#         return x","metadata":{"_uuid":"5b12aec3-1620-4151-8b7c-fe486bea92af","_cell_guid":"f8553ce3-c5a6-4d3f-92a8-ac43a5c99d69","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model BiGRU","metadata":{}},{"cell_type":"code","source":"class ResidualBiGRU(nn.Module):\n    def __init__(self, hidden_size, n_layers=1, bidir=True):\n        super(ResidualBiGRU, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n\n        self.gru = nn.GRU(\n            hidden_size,\n            hidden_size,\n            n_layers,\n            batch_first=True,\n            bidirectional=bidir,\n        )\n        dir_factor = 2 if bidir else 1\n        self.fc1 = nn.Linear(\n            hidden_size * dir_factor, hidden_size * dir_factor * 2\n        )\n        self.ln1 = nn.LayerNorm(hidden_size * dir_factor * 2)\n        self.fc2 = nn.Linear(hidden_size * dir_factor * 2, hidden_size)\n        self.ln2 = nn.LayerNorm(hidden_size)\n\n    def forward(self, x, h=None):\n        res, new_h = self.gru(x, h)\n        # res.shape = (batch_size, sequence_size, 2*hidden_size)\n\n        res = self.fc1(res)\n        res = self.ln1(res)\n        res = nn.functional.relu(res)\n\n        res = self.fc2(res)\n        res = self.ln2(res)\n        res = nn.functional.relu(res)\n\n        # skip connection\n        res = res + x\n\n        return res, new_h\n\nclass MultiResidualBiGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, out_size, n_layers, bidir=True):\n        super(MultiResidualBiGRU, self).__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.out_size = out_size\n        self.n_layers = n_layers\n\n        self.fc_in = nn.Linear(input_size, hidden_size)\n        self.ln = nn.LayerNorm(hidden_size)\n        self.res_bigrus = nn.ModuleList(\n            [\n                ResidualBiGRU(hidden_size, n_layers=1, bidir=bidir)\n                for _ in range(n_layers)\n            ]\n        )\n        self.fc_out = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, h=None):\n        # if we are at the beginning of a sequence (no hidden state)\n        if h is None:\n            # (re)initialize the hidden state\n            h = [None for _ in range(self.n_layers)]\n\n        x = self.fc_in(x)\n        x = self.ln(x)\n        x = nn.functional.relu(x)\n\n        new_h = []\n        for i, res_bigru in enumerate(self.res_bigrus):\n            x, new_hi = res_bigru(x, h[i])\n            new_h.append(new_hi)\n\n        x = self.fc_out(x)\n#         x = F.normalize(x,dim=0)\n        return x, new_h  # log probabilities + hidden states","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:16:22.577141Z","iopub.execute_input":"2023-11-09T13:16:22.577405Z","iopub.status.idle":"2023-11-09T13:16:22.597442Z","shell.execute_reply.started":"2023-11-09T13:16:22.577382Z","shell.execute_reply":"2023-11-09T13:16:22.596713Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Model Version2: with Downsampling & Upsampling","metadata":{}},{"cell_type":"markdown","source":"layer normalization 어디에 써야하고, batchnorm인지 layernorm이 적절할지 어떻게 판단할 수 있을까?","metadata":{}},{"cell_type":"code","source":"class DownSampler(nn.Module):\n\n    def __init__(self, scale, d_model):\n\n        super().__init__()\n        # 5sec to 5min for one step (scale=60)\n        self.cnn1 = nn.Conv1d(d_model, d_model, kernel_size=10, stride=6, padding=2)\n        self.cnn2 = nn.Conv1d(d_model, d_model, kernel_size=7, stride=5, padding=1)\n        self.cnn3 = nn.Conv1d(d_model, d_model, kernel_size=4, stride=2, padding=1)\n        \n    def forward(self, x):\n        # 51840\n        x = self.cnn1(x)\n        x = self.cnn2(x)\n        x = self.cnn3(x)\n\n        return x  # 864.0","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:16:22.598517Z","iopub.execute_input":"2023-11-09T13:16:22.598792Z","iopub.status.idle":"2023-11-09T13:16:22.610681Z","shell.execute_reply.started":"2023-11-09T13:16:22.598768Z","shell.execute_reply":"2023-11-09T13:16:22.609855Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class MyAttention(nn.Module):\n\n    def __init__(self, d_model, d_hid, dropout=0.1):\n        super().__init__()\n        self.scale = d_model ** 0.5\n        self.conv_q = nn.Conv1d(d_model, d_model, kernel_size=6, stride=2, padding=2)\n        self.conv_v = nn.Conv1d(d_model, d_model, kernel_size=6, stride=2, padding=2)\n        self.conv_k = nn.Conv1d(d_model, d_model, kernel_size=6, stride=2, padding=2)\n        self.upsample = nn.Upsample(scale_factor=2)\n        self.conv_attn = nn.Conv1d(d_model, d_model, kernel_size=5, stride=1, padding=2)\n\n        self.attn_dropout = nn.Dropout(dropout)\n        self.layer_norm1 = nn.LayerNorm(d_model, eps=1e-6)  # maybe batchnorm\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.w_1 = nn.Linear(d_model, d_hid)  # params= 16384\n        self.w_2 = nn.Linear(d_hid, d_model)  # params= 16384\n        self.layer_norm2 = nn.LayerNorm(d_model, eps=1e-6)\n        self.dropout2 = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        # x(B Seq D)\n        residual = x.clone()  # ?\n        q = self.conv_q(x)\n        k = self.conv_k(x)\n        v = self.conv_v(x)\n        attn = torch.matmul(k/self.scale, q.transpose(-1,-2))\n        attn = self.attn_dropout(F.softmax(attn, dim=1))\n        x = torch.matmul(attn, v)  # (Lk_sm Lq)*(Lv d) ?\n        x = self.dropout1(x)\n        x = self.layer_norm1(x.transpose(-1,-2)).transpose(-1,-2) # !\n        x = self.upsample(x)\n        x = self.conv_attn(x)\n        x += residual\n\n        residual = x.clone()\n        x = self.w_2(F.relu(self.w_1(x.transpose(-1,-2))))\n        x = self.dropout2(x).transpose(-1,-2)\n        x += residual\n        x = self.layer_norm2(x.transpose(-1,-2)).transpose(-1,-2)\n        # x(B Seq D)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:16:22.611639Z","iopub.execute_input":"2023-11-09T13:16:22.611883Z","iopub.status.idle":"2023-11-09T13:16:22.624648Z","shell.execute_reply.started":"2023-11-09T13:16:22.611861Z","shell.execute_reply":"2023-11-09T13:16:22.623951Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class UpSampler(nn.Module):\n    def __init__(self, scale, d_model):\n        super().__init__()\n        self.upsample1 = nn.Upsample(scale_factor=5)\n        self.upsample2 = nn.Upsample(scale_factor=3)\n        self.upsample3 = nn.Upsample(scale_factor=2)\n        self.upsample4 = nn.Upsample(scale_factor=2)\n        self.conv1 = nn.Conv1d(in_channels=d_model,out_channels=d_model, kernel_size=25,stride=1,padding=12)\n        self.conv2 = nn.Conv1d(in_channels=d_model,out_channels=d_model, kernel_size=15,stride=1,padding=7)\n        self.conv3 = nn.Conv1d(in_channels=d_model,out_channels=d_model, kernel_size=11,stride=1,padding=5)\n        self.conv4 = nn.Conv1d(in_channels=d_model,out_channels=d_model, kernel_size=11,stride=1,padding=5)\n\n        \n    def forward(self, x):\n        x = self.conv1(self.upsample1(x))\n        x = self.conv2(self.upsample2(x))\n        x = self.conv3(self.upsample3(x))\n        x = self.conv4(self.upsample4(x))\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:16:22.625638Z","iopub.execute_input":"2023-11-09T13:16:22.625898Z","iopub.status.idle":"2023-11-09T13:16:22.639431Z","shell.execute_reply.started":"2023-11-09T13:16:22.625875Z","shell.execute_reply":"2023-11-09T13:16:22.638633Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class MyModel(nn.Module):\n    def __init__(self,in_c, out_c, scale, d_model, d_hid, n_blks):\n        super().__init__()\n        self.fc_input = nn.Linear(in_c, d_model)\n        self.enc_blks = nn.ModuleList([\n            nn.Sequential(DownSampler(scale, d_model), \n                          MyAttention(d_model, d_hid),\n                          UpSampler(scale, d_model) ) \n            for _ in range(n_blks)])\n                \n        self.fc_output = nn.Linear(d_model, out_c)\n    \n    def forward(self, x):\n        x = self.fc_input(x)\n        x = x.transpose(-1,-2)\n        for enc_blk in self.enc_blks:\n            x = enc_blk(x)\n        x = self.fc_output(x.transpose(-1,-2))\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:16:22.640320Z","iopub.execute_input":"2023-11-09T13:16:22.640557Z","iopub.status.idle":"2023-11-09T13:16:22.654702Z","shell.execute_reply.started":"2023-11-09T13:16:22.640536Z","shell.execute_reply":"2023-11-09T13:16:22.654014Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"torch.Size([2, 64, 864]) in downsampler out shape  \ntorch.Size([2, 64, 51840]) in upsampler out shape  \ntorch.Size([2, 64, 864]) in downsampler out shape  \ntorch.Size([2, 64, 51840]) in upsampler out shape  \ntorch.Size([2, 64, 864]) in downsampler out shape  \ntorch.Size([2, 64, 51840]) in upsampler out shape  \nbe fc_output,  torch.Size([2, 64, 51840])  \nout shape torch.Size([2, 51840, 2])","metadata":{}},{"cell_type":"code","source":"# class MyModel_out_zero(nn.Module):\n#     def __init__(self, in_c, out_c, scale, d_model, d_hid, n_blks):\n#         super().__init__()\n#         self.fc_input = nn.Linear(in_c, d_model)\n#         self.enc_blks = nn.ModuleList([\n#             nn.Sequential(DownSampler(scale, d_model), \n#                           MyAttention(d_model, d_hid),\n#                           UpSampler(scale, d_model) ) \n#             for _ in range(n_blks)])\n                \n#         self.fc_output = nn.Linear(d_model, out_c)\n    \n#     def forward(self, x):\n#         x = self.fc_input(x)\n#         x = x.transpose(-1,-2)\n#         for enc_blk in self.enc_blks:\n#             x = enc_blk(x)\n#         x = self.fc_output(x.transpose(-1,-2))\n#         return x*0.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"_uuid":"c7d156cb-8af7-4315-92ab-91e495c3072d","_cell_guid":"877e27cf-b415-4325-8d7b-5042d9712c93","trusted":true}},{"cell_type":"markdown","source":"## 1. Read from csv at each call.","metadata":{"_uuid":"bd41a2b9-6500-4781-998a-d921b8bb7c36","_cell_guid":"611f8ba3-529e-4f83-a9a2-108e40f00743","trusted":true}},{"cell_type":"code","source":"class SleepDatasetCSV(Dataset):\n    \n    def __init__(self, input_paths, gen_feat, feat_type, mode):\n        \"\"\"\n        gen_feat<bool>: rolling mean, std\n        feat_type<str>: enmo, anglez\n        mode: 'trainval', test'\n        \"\"\"\n        self.gen_feat = gen_feat\n        self.feat_type = feat_type\n        self.mode = mode\n        self.input_paths = input_paths\n        \n    def __len__(self):\n        return len(self.input_paths)\n    \n    def generate_features(self, X):\n        for r in CONFIG.ROLLING_RANGES:\n            tmp_feat = X[f'{self.feat_type}'].rolling(r, center=True)\n            X[f'{self.feat_type}_mean_{r}'] = tmp_feat.mean()\n            X[f'{self.feat_type}_std_{r}'] = tmp_feat.std()\n        return X.drop(columns=['onset','wakeup']).fillna(0)\n    \n    def __getitem__(self, index):\n        path = self.input_paths[index]\n        XY = pd.read_csv(path)\n        \n        if self.gen_feat:\n            X = self.generate_features(XY).to_numpy()\n        else:\n            X = XY[self.feat_type].copy().to_numpy()\n            \n        if self.mode=='trainval':\n            Y = XY[['onset','wakeup']].copy().to_numpy()\n            gc.collect()\n            return X,Y\n        elif self.mode=='test':\n            gc.collect()\n            return X","metadata":{"_uuid":"7221bb8f-d151-4ac6-aafb-eeec7ca8773e","_cell_guid":"6b734299-abbc-4638-8aae-94ba4d06cf70","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kfold_split(input_paths, K):\n    splited_input_paths=[]\n    for i in range(K):\n        st=i*(len(input_paths)//K)\n        ed=(i+1)*(len(input_paths)//K)\n        if i==K-1:\n            splited_input_paths.append(input_paths[st:])\n        else:\n            splited_input_paths.append(input_paths[st:ed])\n    \n    return splited_input_paths","metadata":{"_uuid":"7debd7e1-6ebd-4a67-b51d-95c689479d12","_cell_guid":"2187ca1e-cbb9-4c52-afaf-503d83282f93","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train valid K-Fold Cross Validation\nenmo_input_paths = glob.glob(f'{CONFIG.INPUT_DIR}/enmo/*.csv')\nanglez_input_paths = glob.glob(f'{CONFIG.INPUT_DIR}/anglez/*.csv')\n\nif CONFIG.SHUFFLE:\n    random.shuffle(enmo_input_paths) # shuffle\n    random.shuffle(anglez_input_paths) # shuffle\n\n# split into Kfolds\nenmo_kfold_paths = kfold_split(enmo_input_paths, CONFIG.FOLDS)\nanglez_kfold_paths = kfold_split(anglez_input_paths, CONFIG.FOLDS)","metadata":{"_uuid":"c1d6c6b2-f569-4069-a08a-bdcb1e879334","_cell_guid":"fccbd0ce-cc12-4b00-9bbf-46231e395f17","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# enmo train, valid loaders\ntrain_dl = DataLoader(SleepDatasetCSV(enmo_kfold_paths[1]+enmo_kfold_paths[2]+enmo_kfold_paths[3]+enmo_kfold_paths[4], \n                                            gen_feat=True, feat_type='enmo', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\neval_dl = DataLoader(SleepDatasetCSV(enmo_kfold_paths[0], \n                                            gen_feat=True, feat_type='enmo', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n\n\n\n# enmo_train_dl0 = DataLoader(SleepDatasetCSV(enmo_kfold_paths[1]+enmo_kfold_paths[2]+enmo_kfold_paths[3]+enmo_kfold_paths[4], \n#                                             gen_feat=True, feat_type='enmo', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n# enmo_valid_dl0 = DataLoader(SleepDatasetCSV(enmo_kfold_paths[0], \n#                                             gen_feat=True, feat_type='enmo', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n\n# enmo_train_dl1 = DataLoader(SleepDatasetCSV(enmo_kfold_paths[0]+enmo_kfold_paths[2]+enmo_kfold_paths[3]+enmo_kfold_paths[4], \n#                                             gen_feat=True, feat_type='enmo', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n# enmo_valid_dl1 = DataLoader(SleepDatasetCSV(enmo_kfold_paths[1], \n#                                             gen_feat=True, feat_type='enmo', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n\n# enmo_train_dl2 = DataLoader(SleepDatasetCSV(enmo_kfold_paths[0]+enmo_kfold_paths[1]+enmo_kfold_paths[3]+enmo_kfold_paths[4], \n#                                             gen_feat=True, feat_type='enmo', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n# enmo_valid_dl2 = DataLoader(SleepDatasetCSV(enmo_kfold_paths[2], \n#                                             gen_feat=True, feat_type='enmo', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n\n# enmo_train_dl3 = DataLoader(SleepDatasetCSV(enmo_kfold_paths[0]+enmo_kfold_paths[1]+enmo_kfold_paths[2]+enmo_kfold_paths[4], \n#                                             gen_feat=True, feat_type='enmo', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n# enmo_valid_dl3 = DataLoader(SleepDatasetCSV(enmo_kfold_paths[3], \n#                                             gen_feat=True, feat_type='enmo', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n\n# enmo_train_dl4 = DataLoader(SleepDatasetCSV(enmo_kfold_paths[0]+enmo_kfold_paths[1]+enmo_kfold_paths[2]+enmo_kfold_paths[3], \n#                                             gen_feat=True, feat_type='enmo', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n# enmo_valid_dl4 = DataLoader(SleepDatasetCSV(enmo_kfold_paths[4], \n#                                             gen_feat=True, feat_type='enmo', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n\n\n# # anglez train, valid loaders\n# anglez_train_dl0 = DataLoader(SleepDatasetCSV(anglez_kfold_paths[1]+anglez_kfold_paths[2]+anglez_kfold_paths[3]+anglez_kfold_paths[4], \n#                                             gen_feat=True, feat_type='anglez', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n# anglez_valid_dl0 = DataLoader(SleepDatasetCSV(anglez_kfold_paths[0], \n#                                             gen_feat=True, feat_type='anglez', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n\n# anglez_train_dl1 = DataLoader(SleepDatasetCSV(anglez_kfold_paths[0]+anglez_kfold_paths[2]+anglez_kfold_paths[3]+anglez_kfold_paths[4], \n#                                             gen_feat=True, feat_type='anglez', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n# anglez_valid_dl1 = DataLoader(SleepDatasetCSV(anglez_kfold_paths[1], \n#                                             gen_feat=True, feat_type='anglez', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n\n# anglez_train_dl2 = DataLoader(SleepDatasetCSV(anglez_kfold_paths[0]+anglez_kfold_paths[1]+anglez_kfold_paths[3]+anglez_kfold_paths[4], \n#                                             gen_feat=True, feat_type='anglez', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n# anglez_valid_dl2 = DataLoader(SleepDatasetCSV(anglez_kfold_paths[2], \n#                                             gen_feat=True, feat_type='anglez', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n\n# anglez_train_dl3 = DataLoader(SleepDatasetCSV(anglez_kfold_paths[0]+anglez_kfold_paths[1]+anglez_kfold_paths[2]+anglez_kfold_paths[4], \n#                                             gen_feat=True, feat_type='anglez', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n# anglez_valid_dl3 = DataLoader(SleepDatasetCSV(anglez_kfold_paths[3], \n#                                             gen_feat=True, feat_type='anglez', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n\n# anglez_train_dl4 = DataLoader(SleepDatasetCSV(anglez_kfold_paths[0]+anglez_kfold_paths[1]+anglez_kfold_paths[2]+anglez_kfold_paths[3], \n#                                             gen_feat=True, feat_type='anglez', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n# anglez_valid_dl4 = DataLoader(SleepDatasetCSV(anglez_kfold_paths[4], \n#                                             gen_feat=True, feat_type='anglez', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)","metadata":{"_uuid":"0978bbd3-9a1b-467e-85f0-911ea62428bc","_cell_guid":"f2e1f776-6ec3-40dd-891a-74dd8e10c318","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for x,y in tqdm(enmo_train_dl0):\n# #     print(x.shape, y.shape)\n# # default 2.55 s/it","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# enmo_train_nofeat = DataLoader(SleepDatasetCSV(glob.glob(f'{CONFIG.INPUT_DIR}/enmo/*.csv'), \n#                                             gen_feat=False, feat_type='enmo', mode='trainval'), batch_size=CONFIG.BS, shuffle=True)\n# for x,y in tqdm(enmo_train_nofeat):\n#     print(x.shape,y.shape)\n# # without feature generation, 4.62s/it","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. load on the RAM","metadata":{}},{"cell_type":"code","source":"def kfold_split(input_paths, K):\n    splited_input_paths=[]\n    for i in range(K):\n        st=i*(len(input_paths)//K)\n        ed=(i+1)*(len(input_paths)//K)\n        if i==K-1:\n            splited_input_paths.append(input_paths[st:])\n        else:\n            splited_input_paths.append(input_paths[st:ed])\n    \n    return splited_input_paths\n\n### how about store it on RAM?\nenmo_input_paths = glob.glob(f'{CONFIG.INPUT_DIR}/enmo/*.csv')\nif CONFIG.SHUFFLE:\n    random.shuffle(enmo_input_paths) # shuffle\nenmo_kfold_paths = kfold_split(enmo_input_paths, CONFIG.FOLDS)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:16:26.866758Z","iopub.execute_input":"2023-11-09T13:16:26.867789Z","iopub.status.idle":"2023-11-09T13:16:27.184481Z","shell.execute_reply.started":"2023-11-09T13:16:26.867747Z","shell.execute_reply":"2023-11-09T13:16:27.183722Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"enmo_kfold_paths_train=[]\nenmo_kfold_paths_eval=[]\nenmo_kfold_paths_train += enmo_kfold_paths[1]\nenmo_kfold_paths_train += enmo_kfold_paths[2]\nenmo_kfold_paths_train += enmo_kfold_paths[3]\nenmo_kfold_paths_train += enmo_kfold_paths[4]\nenmo_kfold_paths_eval += enmo_kfold_paths[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:16:27.421387Z","iopub.execute_input":"2023-11-09T13:16:27.422091Z","iopub.status.idle":"2023-11-09T13:16:27.426759Z","shell.execute_reply.started":"2023-11-09T13:16:27.422059Z","shell.execute_reply":"2023-11-09T13:16:27.425864Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"len(enmo_kfold_paths_train)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:16:27.808104Z","iopub.execute_input":"2023-11-09T13:16:27.808402Z","iopub.status.idle":"2023-11-09T13:16:27.814148Z","shell.execute_reply.started":"2023-11-09T13:16:27.808376Z","shell.execute_reply":"2023-11-09T13:16:27.813192Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"5721"},"metadata":{}}]},{"cell_type":"code","source":"enmo_inputs_train=[]\nenmo_targets_train=[]\nattach_hours=8\ndef generate_features_enmo(xy):\n    for r in [17, 33, 65]:\n        tmp_feat = xy['enmo'].rolling(r, center=True)\n        xy[f'enmo_mean_{r}'] = tmp_feat.mean()\n        xy[f'enmo_std_{r}'] = tmp_feat.std()\n    return xy.drop(columns=['onset','wakeup']).fillna(0)\n\n# TRAIN\nprint('about 26 min')\nfor i, path in tqdm(enumerate(enmo_kfold_paths_train), total=len(enmo_kfold_paths_train)):\n    st=(24-attach_hours)*60*12\n    ed=-(24-attach_hours)*60*12\n    xy = pd.read_csv(path)\n    x = generate_features_enmo(xy)\n    enmo_inputs_train.append(x.to_numpy()[st:ed])\n    enmo_targets_train.append(xy[['onset','wakeup']].to_numpy()[st:ed])\n    del x, xy; gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:16:29.479528Z","iopub.execute_input":"2023-11-09T13:16:29.479845Z","iopub.status.idle":"2023-11-09T13:44:36.451939Z","shell.execute_reply.started":"2023-11-09T13:16:29.479820Z","shell.execute_reply":"2023-11-09T13:44:36.451039Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"about 26 min\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5721 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e6a496550be431498f3889a1486e5f0"}},"metadata":{}}]},{"cell_type":"code","source":"# EVAL\nenmo_inputs_eval=[]\nenmo_targets_eval=[]\nattach_hours=8\nprint('about 6.5 min')\nfor i, path in tqdm(enumerate(enmo_kfold_paths_eval), total=len(enmo_kfold_paths_eval)):\n    st=(24-attach_hours)*60*12\n    ed=-(24-attach_hours)*60*12\n    xy = pd.read_csv(path)[st:ed]\n    x = generate_features(xy)\n    enmo_inputs_eval.append(x.to_numpy())\n    enmo_targets_eval.append(xy[['onset','wakeup']].to_numpy())\n    del xy, x; gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SleepDatasetTRAIN(Dataset):\n    def __init__(self):\n        pass\n    def __len__(self):\n        return len(enmo_inputs_train)\n    def __getitem__(self, index):\n        return enmo_inputs_train[index], enmo_targets_train[index]\n    \nclass SleepDatasetEVAL(Dataset):\n    def __init__(self):\n        pass\n    def __len__(self):\n        return len(enmo_inputs_eval)\n    def __getitem__(self, index):\n        return enmo_inputs_eval[index], enmo_targets_eval[index]\n\ntrain_dl = DataLoader(SleepDatasetTRAIN(), batch_size=CONFIG.BS, shuffle=True)\n# eval_dl = DataLoader(SleepDatasetEVAL(), batch_size=CONFIG.BS, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:44:36.453717Z","iopub.execute_input":"2023-11-09T13:44:36.454030Z","iopub.status.idle":"2023-11-09T13:44:36.461130Z","shell.execute_reply.started":"2023-11-09T13:44:36.454004Z","shell.execute_reply":"2023-11-09T13:44:36.460157Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Train and Evaluate: ENMO","metadata":{"_uuid":"93d7a0d4-4485-4146-a506-d466fdfa85e8","_cell_guid":"9f0cac3d-5412-4aa8-89eb-de89fc59d46e","trusted":true}},{"cell_type":"code","source":"# loss function - regression, outliers represent anomalies that should be detected. -> use MSE type of regression loss\nclass FocalLoss(nn.Module):\n    \n    def __init__(self, weight=None, seize_average=True, alpha=1., gamma=2.):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    \n    def forward(self, inputs, targets):\n        inputs = F.sigmoid(inputs)  # make outputs btw 0~1\n        \n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        BCE_EXP = torch.exp(-BCE)\n        focal_loss = self.alpha * (1-BCE_EXP)**self.gamma * BCE\n        \n        return focal_loss.mean()","metadata":{"_uuid":"caf748ba-2718-429a-8d53-9467a8864c93","_cell_guid":"a1b4b9b7-5072-4e2f-82bd-51044d62ecc4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-09T13:44:36.462300Z","iopub.execute_input":"2023-11-09T13:44:36.462630Z","iopub.status.idle":"2023-11-09T13:44:36.476817Z","shell.execute_reply.started":"2023-11-09T13:44:36.462599Z","shell.execute_reply":"2023-11-09T13:44:36.475954Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# weight decay\ndef add_weight_decay(model, weight_decay=1e-5, skip_list=()):\n    decay=[]\n    no_decay=[]\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n        if len(param.shape)==1 or np.any([v in name.lower() for v in skip_list]):\n            no_decay.append(param)\n        else:\n            decay.append(param)\n    return [\n        {'params': no_decay, 'weight_decay':0.},\n        {'params': decay, 'weight_decay': weight_decay}\n    ]","metadata":{"_uuid":"8be0a042-80e2-4018-9f03-47b66548ecd1","_cell_guid":"c2fcb822-39be-4c3c-a20d-d78566b8f101","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-09T13:44:36.479429Z","iopub.execute_input":"2023-11-09T13:44:36.479998Z","iopub.status.idle":"2023-11-09T13:44:36.488557Z","shell.execute_reply.started":"2023-11-09T13:44:36.479973Z","shell.execute_reply":"2023-11-09T13:44:36.487691Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{"_uuid":"57705197-51f7-40dc-a9ba-e473385b6e70","_cell_guid":"699ff763-798e-4ccd-a29d-f30efa519c59","trusted":true}},{"cell_type":"code","source":"def train(model, train_loader, optimizer):\n    model.train()\n    train_loss=0\n    with tqdm(train_loader, leave=True) as pbar:\n        for step, (x,y) in enumerate(pbar):\n            x = x.to(torch.float32).to(device)\n            y = y.to(torch.float32).to(device)\n            optimizer.zero_grad()\n            pred, _ = model(x)\n            loss = criterion(pred,y)\n            train_loss += loss.item()\n            \n            loss.backward()\n            optimizer.step()\n\n            pbar.set_postfix(\n                    OrderedDict(\n                        loss=f'{loss.item():.6f}',\n                        lr=f'{optimizer.param_groups[0][\"lr\"]:.3e}'\n                    )\n                )\n        train_loss /= len(train_loader)\n    return train_loss","metadata":{"_uuid":"63e09d70-811e-4e3a-9fe4-af685d074f31","_cell_guid":"e41be20a-7527-4e37-a5d7-157324f26816","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-09T13:44:36.489666Z","iopub.execute_input":"2023-11-09T13:44:36.490002Z","iopub.status.idle":"2023-11-09T13:44:36.502276Z","shell.execute_reply.started":"2023-11-09T13:44:36.489970Z","shell.execute_reply":"2023-11-09T13:44:36.501378Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, val_loader):\n    model.eval()\n    val_loss=0\n    with torch.no_grad():\n        with tqdm(val_loader, leave=True) as pbar:\n            for x,y in pbar:\n                x=x.to(torch.float32).to(device)\n                y=y.to(torch.float32).to(device)\n                pred,_ = model(x)\n                loss = criterion(pred,y)\n                val_loss += loss.item()\n                \n                pbar.set_postfix(\n                        OrderedDict(\n                            loss=f'{loss.item():.6f}',\n                            lr=f'{optimizer.param_groups[0][\"lr\"]:.3e}'\n                        )\n                    )\n    val_loss /= len(val_loader)\n    \n    return val_loss","metadata":{"_uuid":"544465a5-1304-4203-80f4-1ba57c744730","_cell_guid":"e9dde0be-1cd1-45a1-916f-f4398cd07abe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-09T13:44:36.503409Z","iopub.execute_input":"2023-11-09T13:44:36.503734Z","iopub.status.idle":"2023-11-09T13:44:36.518548Z","shell.execute_reply.started":"2023-11-09T13:44:36.503704Z","shell.execute_reply":"2023-11-09T13:44:36.517743Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Train folds !\ndef train_folds(model, train_dl, valid_dl, optimizer, scheduler, epochs, fold_num):\n    os.makedirs('./model',exist_ok=True)\n    history = {\n        'train_loss': [],\n        'valid_loss': [],\n        'lr': [],\n    }\n    best_valid_loss = 1e5\n    for epoch in range(epochs):\n        \n        train_loss = train(model, train_dl, optimizer)\n        valid_loss = evaluate(model, valid_dl)\n\n        history['train_loss'].append(train_loss)\n        history['valid_loss'].append(valid_loss)\n        history['lr'].append(optimizer.param_groups[0][\"lr\"])\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(\n                model.state_dict(),\n                os.path.join('./model/', f\"model_best_fold-{fold_num}.pth\")\n            )\n        print(\n            f\"epoch{epoch+1} -- \",\n            f\"train_loss = {train_loss:.6f} -- \",\n            f\"valid_loss = {valid_loss:.6f}\",\n        )\n        if scheduler!=None:\n            scheduler.step()","metadata":{"_uuid":"3a28ad16-d677-4be3-963d-61c220753659","_cell_guid":"81274032-e6d9-47e4-843c-49b6aae50c01","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-09T13:44:36.519654Z","iopub.execute_input":"2023-11-09T13:44:36.519918Z","iopub.status.idle":"2023-11-09T13:44:36.531479Z","shell.execute_reply.started":"2023-11-09T13:44:36.519881Z","shell.execute_reply":"2023-11-09T13:44:36.530765Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def get_scheduler(optimizer):\n    steps = len(train_dl)*CONFIG.EPOCHS\n    warmup_steps = int(steps*CONFIG.WARMUP_PROP)\n    print('steps, warmup_steps: ',steps, warmup_steps)\n    scheduler = get_cosine_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=warmup_steps,\n                                                num_training_steps=steps,\n                                                num_cycles=0.5)\n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:44:36.532535Z","iopub.execute_input":"2023-11-09T13:44:36.532838Z","iopub.status.idle":"2023-11-09T13:44:36.546150Z","shell.execute_reply.started":"2023-11-09T13:44:36.532813Z","shell.execute_reply":"2023-11-09T13:44:36.545328Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"\"\"\"model version 1\"\"\"\n# model = Encoder(\n#     n_blocks=CONFIG.N_BLKS,\n#     in_channels=CONFIG.IN_CHANNEL,\n#     out_classes=2,\n#     d_model=CONFIG.D_MODEL,\n#     d_hid=CONFIG.D_MODEL*4,\n#     dropout=CONFIG.DROPOUT,\n#     agg_scale=None).to(device)\n# # 915 MB","metadata":{"_uuid":"e433d89e-1af1-440d-841d-f94442c5299e","_cell_guid":"1597dcc7-8624-413d-8a37-fdfa8c8c0590","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"\"\"model version 2\"\"\"\n# model = MyModel(in_c=7, out_c=2, scale=60, d_model=64, d_hid=64*2, n_blks=3).to(device)\n# # model = MyModel_out_zero(in_c=7, out_c=2, scale=60, d_model=64, d_hid=64*2, n_blks=3).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"model Bigru\"\"\"\nmodel = MultiResidualBiGRU(input_size=7, hidden_size=64,out_size=2,n_layers=2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:44:36.547099Z","iopub.execute_input":"2023-11-09T13:44:36.547345Z","iopub.status.idle":"2023-11-09T13:44:43.723617Z","shell.execute_reply.started":"2023-11-09T13:44:36.547324Z","shell.execute_reply":"2023-11-09T13:44:43.722806Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# inp = torch.unsqueeze(torch.tensor(enmo_inputs_train[0]).to(torch.float32).to(device), dim=0)\n# # print(inp.shape)\n# out=model(inp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer_parameters = add_weight_decay(model, weight_decay=CONFIG.WD, skip_list=['bias'])\noptimizer = AdamW(optimizer_parameters, lr=CONFIG.LR, eps=1e-6, betas=(0.9, 0.999))\nprint('lr ', CONFIG.LR)\n# scheduler = get_scheduler(optimizer)\ncriterion = FocalLoss()","metadata":{"_uuid":"16a595c3-965d-4039-a0f9-05c5896e9e23","_cell_guid":"03cd0db4-8467-40a7-83c6-6677e015076d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-11-09T13:44:43.725852Z","iopub.execute_input":"2023-11-09T13:44:43.726147Z","iopub.status.idle":"2023-11-09T13:44:43.732247Z","shell.execute_reply.started":"2023-11-09T13:44:43.726122Z","shell.execute_reply":"2023-11-09T13:44:43.731408Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"lr  0.0005\n","output_type":"stream"}]},{"cell_type":"code","source":"# optimizer = AdamW(optimizer_parameters, lr=1000, eps=1e-6, betas=(0.9, 0.999))\ntrain(model, train_dl, optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T13:56:16.814476Z","iopub.execute_input":"2023-11-09T13:56:16.814833Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1431 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4da0d1a967e4b99836131210eab6ecb"}},"metadata":{}}]},{"cell_type":"code","source":"# # out check\n# inp = torch.unsqueeze(torch.tensor(enmo_inputs_train[0]).to(torch.float32).to(device), dim=0)\n# target = torch.unsqueeze(torch.tensor(enmo_targets_train[0]).to(torch.float32).to(device), dim=0)\n\n# # print(inp.shape)\n# model.eval()\n# out=model(inp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"loss 가 0.1732868105173111에서 낮아지질 않는다. 몇 step 진행하지도 않았는데. 뭔가가 grdient가 전부 0이 되거나, output들이 죽는 현상이 발생하고있음.\n\nlr을 1000으로 학습해보니 loss는 0.3612673056125641나옴.\n\n어떤 이유에서인지 gradient vanishing이 극복되지 않는 minima에 갇힌것같음.\n\n\n- 모델의 output이 모두 0는 아님.","metadata":{}},{"cell_type":"markdown","source":"BiGRU 로 학습시켜보니깐, loss가 0.17328 밑으로 줄어들음. 내가 모델링을 잘 못한 것 같음.\n\nloss=0.173467 아직 1epoch도 학습 안했긴 했지만, 그렇게 크게 차이나진 않는듯 함.","metadata":{}},{"cell_type":"markdown","source":"loss = 0.173296 으로 고정됨 모델을 바꿔도.\n\n데이터 문제인가?? loss 문제인가?","metadata":{}},{"cell_type":"markdown","source":"loss?\n\ntarget? (segmentation?)\n\nmodel? (non-linearity)\n- 다른 모델도 성능이 안나와야하겠지.\n","metadata":{}},{"cell_type":"code","source":"# TRAIN - fold 0\ntrain_folds(model=model, \n            train_dl=train_dl, \n            valid_dl=eval_dl, \n            optimizer=optimizer, \n            scheduler=None,\n            epochs=CONFIG.EPOCHS,\n            fold_num=0)\n\n# loss=0.173287 for train\n# loss=0.173287 for eval","metadata":{"_uuid":"90485058-3262-42d1-8c50-6c5245a190e7","_cell_guid":"6e0636ff-0f3a-44c4-b235-9f31f2d61b3a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Version 1","metadata":{}},{"cell_type":"markdown","source":"2.85s/it - BS8  \n분명 더 forward(), backward(), dataloading 시간 줄일 방향 있을거야.\n\ntrain_loss = 0.173  \nval_loss = 0.17344413714368917","metadata":{}},{"cell_type":"markdown","source":"#### Model Version 2\n\n2.71s/it - BS8  \ntrain_loss =  \nval_loss =  \n","metadata":{}},{"cell_type":"markdown","source":"# Plot History","metadata":{"_uuid":"7a75e222-02bd-44ab-82c4-20c0ccc3614f","_cell_guid":"9c794eac-8e01-4781-b3fc-fc06305422f0","trusted":true}},{"cell_type":"code","source":"def plot_history(history, model_path=\".\", show=True):\n    epochs = range(1, len(history[\"train_loss\"]) + 1)\n\n    plt.figure()\n    plt.plot(epochs, history[\"train_loss\"], label=\"Training Loss\")\n    plt.plot(epochs, history[\"valid_loss\"], label=\"Validation Loss\")\n    plt.yscale('log')\n    plt.title(\"Loss evolution\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(os.path.join(model_path, \"loss_evo.png\"))\n    if show:\n        plt.show()\n    plt.close()\n\n#     plt.figure()\n#     plt.plot(epochs, history[\"valid_mAP\"])\n#     plt.title(\"Validation mAP evolution\")\n#     plt.xlabel(\"Epochs\")\n#     plt.ylabel(\"mAP\")\n#     plt.savefig(os.path.join(model_path, \"mAP_evo.png\"))\n#     if show:\n#         plt.show()\n#     plt.close()\n\n    plt.figure()\n    plt.plot(epochs, history[\"lr\"])\n    plt.title(\"Learning Rate evolution\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"LR\")\n    plt.savefig(os.path.join(model_path, \"lr_evo.png\"))\n    if show:\n        plt.show()\n    plt.close()","metadata":{"_uuid":"c9097fb4-7a9d-4ea6-bdf1-5f024b504a32","_cell_guid":"389694b2-8c3b-4a9b-a16b-0e70d13d6d29","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history, model_path=model_path)\nhistory_path = os.path.join(model_path, \"history.json\")\nwith open(history_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(history, f, ensure_ascii=False, indent=4)","metadata":{"_uuid":"45e55325-69a0-466a-b463-c4c5175f1b6d","_cell_guid":"cd83d375-bf04-404b-976a-60202c4f01f6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"86f3ebfe-4a06-45e8-a335-c4f730971c47","_cell_guid":"21a9490a-0762-4008-b5ea-d261ec80bcac","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}