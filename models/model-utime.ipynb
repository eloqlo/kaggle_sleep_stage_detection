{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":53666,"databundleVersionId":6589269,"sourceType":"competition"},{"sourceId":6671455,"sourceType":"datasetVersion","datasetId":3849233}],"dockerImageVersionId":30559,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TRAIN","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport datetime\nimport pandas as pd\nimport numpy as np\nimport os\nimport gc\n\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"execution":{"iopub.status.busy":"2023-11-09T12:22:30.043106Z","iopub.execute_input":"2023-11-09T12:22:30.043409Z","iopub.status.idle":"2023-11-09T12:22:37.900665Z","shell.execute_reply.started":"2023-11-09T12:22:30.043381Z","shell.execute_reply":"2023-11-09T12:22:37.899700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CONFIGs","metadata":{}},{"cell_type":"code","source":"# 데이터 분포 고려, 적절한 길이이면서 + 모델 layer 특성 상 중간에서 zero_padding 일어나지 않는 길이.\nMAX_SEQ_LEN = 499200\nN_CLASSES = 1\nN_CHANNELS = 2\n\nK_SPLITS = 5   # Number of folds for cross validation.\n\nEPOCHS=100\nBATCH_SIZE = 4\nLEARNING_RATE = 1e-3\n\n# Prediction threshold ! over this will be considered as sleeping steps.\n# it is equal to model's threshold.(maybe, need to be checked)\nTHRESHOLD = 0.5","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:13:27.458402Z","iopub.execute_input":"2023-10-23T11:13:27.458950Z","iopub.status.idle":"2023-10-23T11:13:27.465601Z","shell.execute_reply.started":"2023-10-23T11:13:27.458920Z","shell.execute_reply":"2023-10-23T11:13:27.464080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:13:27.467067Z","iopub.execute_input":"2023-10-23T11:13:27.467555Z","iopub.status.idle":"2023-10-23T11:13:27.488693Z","shell.execute_reply.started":"2023-10-23T11:13:27.467517Z","shell.execute_reply":"2023-10-23T11:13:27.487752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DATA PREPARATION","metadata":{}},{"cell_type":"code","source":"def series_cleaner(series, ID, clean_type=1):\n    \"\"\"\n    child method for below 'series_segmentation_preprocess_cleaning' method.\n    \"\"\"\n    to_clean_indices=[]\n    if clean_type==1:\n        to_clean_indices+=[165, 200, 4, 17, 24, 53, 65, 100]\n\n        if ID==to_clean_indices[0]:\n            return series[:230000]\n        elif ID==to_clean_indices[1]:\n            return series[150000:400000]\n        elif ID==to_clean_indices[2]:\n            return series[:140000]\n        elif ID==to_clean_indices[3]:\n            return series[:75000]\n        elif ID==to_clean_indices[4]:\n            return series[:90000]\n        elif ID==to_clean_indices[5]:\n            return series[:200000]\n        elif ID==to_clean_indices[6]:\n            return series   # train whole.\n        elif ID==to_clean_indices[7]:\n            return series[:300000]\n        \n        # else\n        return series\n\n# Premise: y_true is one hot vector\ndef series_segmentation_preprocess_cleaning(series, clean_type=1):\n    \"\"\"\n    INPUTS\n        - series: preprocessed series\n        - clean_type\n            0: do nothing.\n            1: drop unlabeled area which don't have pattern. (suspicious ones)\n\n    OUTPUTS\n        - X: (N, MAX_SEQ_LEN, 2) truncated, padded series\n    \"\"\"\n\n    # for my preprocessed data only\n    X = []\n    X_len_type=[]   # for event preprocessing\n    total_id_num = int(series['id_index'].iloc[-1]) + 1\n    for i in range(total_id_num):\n        if i==163:\n            # do not train id==163. too noisy?\n            # TODO : need check if model is good to have this data.\n            X_len_type.append(100)  # 100 means pass.\n            continue\n        series_per_id = series.loc[series['id_index'] == i].drop(['id_index'], axis=1)\n\n        # if using data cleaning\n        if clean_type != 0:\n            series_per_id = series_cleaner(series_per_id, i, clean_type)\n\n        # fit series into model's config\n        if len(series_per_id) <= MAX_SEQ_LEN:\n            seq1 = series_per_id.to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq1)\n            seq1 = np.pad(seq1, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            X += [seq1]\n            len_type=0\n            X_len_type.append(len_type)\n        elif MAX_SEQ_LEN < len(series_per_id) <= 2*MAX_SEQ_LEN:\n            seq1 = series_per_id[:MAX_SEQ_LEN].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq1)\n            seq1 = np.pad(seq1, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            seq2 = series_per_id[MAX_SEQ_LEN:].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq2)\n            seq2 = np.pad(seq2, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            X += [seq1,seq2]\n            len_type=1\n            X_len_type.append(len_type)\n        elif 2*MAX_SEQ_LEN < len(series_per_id) <= 3*MAX_SEQ_LEN:\n            seq1 = series_per_id[:MAX_SEQ_LEN].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq1)\n            seq1=np.pad(seq1, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            seq2 = series_per_id[MAX_SEQ_LEN:2*MAX_SEQ_LEN].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq2)\n            seq2=np.pad(seq2, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            seq3 = series_per_id[2*MAX_SEQ_LEN:].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq3)\n            seq3=np.pad(seq3, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            X += [seq1,seq2,seq3]\n            len_type=2\n            X_len_type.append(len_type)\n        elif 3*MAX_SEQ_LEN < len(series_per_id) <= 4*MAX_SEQ_LEN:\n            seq1 = series_per_id[:MAX_SEQ_LEN].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq1)\n            seq1 = np.pad(seq1, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            seq2 = series_per_id[MAX_SEQ_LEN:2*MAX_SEQ_LEN].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq2)\n            seq2 = np.pad(seq2, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            seq3 = series_per_id[2*MAX_SEQ_LEN:3*MAX_SEQ_LEN].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq3)\n            seq3 = np.pad(seq3, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            seq4 = series_per_id[3*MAX_SEQ_LEN:].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq4)\n            seq4 = np.pad(seq4, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            X += [seq1,seq2,seq3,seq4]\n            len_type=3\n            X_len_type.append(len_type)\n        else:\n            assert False, \"MAX_SEQ_LEN > 1,840,000 is yet implemented for this dataset. something you are doing wrong.\"\n\n    return np.array(X), np.array(X_len_type)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:13:27.492293Z","iopub.execute_input":"2023-10-23T11:13:27.492914Z","iopub.status.idle":"2023-10-23T11:13:27.515665Z","shell.execute_reply.started":"2023-10-23T11:13:27.492868Z","shell.execute_reply":"2023-10-23T11:13:27.514530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def event_cleaner(events, ID, clean_type=1):\n    \"\"\"\n    I tried to make labels using event['step']. \n    If series were modified, event['step'] should also be modified.\n    This function does that role.\n    \"\"\"\n\n    if clean_type==1:\n        to_clean_indices = [165, 200, 4, 17, 24, 53, 65, 100]\n        \n        # modify event step\n        if ID==to_clean_indices[1]:\n            events['step'] -= 150000\n            return events\n    # else\n    return events\n\ndef events_segmentation_preprocess_cleaning(events, X_len_type, clean_type=1):\n    \"\"\"\n    events: preprocessed events data\n    X_len_type: from series_preprocess output\n    clean_type: same as 'series_segmentation_preprocess'.\n    \"\"\"\n\n    df_to_gt=[]\n    for ID, ID_len_type in enumerate(X_len_type):\n\n        if ID in [163]:\n            assert ID_len_type != 'pass', \"Internal Error.\"\n            # do not trian id==163. too noisy?\n            # TODO : need check if model is good to have this data.\n            continue\n        labels = events.loc[events['id_index']==ID].reset_index(drop=False)\n\n        # If use cleaning,\n        if clean_type != 0:\n            labels = event_cleaner(labels, ID, clean_type)\n\n        if ID_len_type == 0:\n            mask1 = np.zeros(MAX_SEQ_LEN)\n            for i in range(len(labels)):\n                if labels.loc[i]['event']==0:\n                    start=labels.loc[i]['step']\n                    end=labels.loc[i+1]['step']\n                    mask1[start:end]=1\n            df_to_gt+=[mask1]\n\n        elif ID_len_type==1:\n            mask1 = np.zeros(MAX_SEQ_LEN)\n            mask2 = np.zeros(MAX_SEQ_LEN)\n            for i in range(len(labels)):\n                if labels.loc[i]['event']==0:\n                    start=labels.loc[i]['step']   # onset\n                    end=labels.loc[i+1]['step']   # wakeup\n                    if start<MAX_SEQ_LEN and end<MAX_SEQ_LEN:  #1\n                        mask1[start:end]=1\n                    elif start<MAX_SEQ_LEN and end>=MAX_SEQ_LEN:  #2\n                        mask1[start:]=1\n                        mask2[:end-MAX_SEQ_LEN]=1\n                    elif start>=MAX_SEQ_LEN:  #3\n                        mask2[start-MAX_SEQ_LEN:end-MAX_SEQ_LEN]=1\n            df_to_gt+=[mask1,mask2]\n\n        elif ID_len_type==2:\n            mask1 = np.zeros(MAX_SEQ_LEN)\n            mask2 = np.zeros(MAX_SEQ_LEN)\n            mask3 = np.zeros(MAX_SEQ_LEN)\n            for i in range(len(labels)):\n                if labels.loc[i]['event']==0:\n                    start=labels.loc[i]['step']   # onset\n                    end=labels.loc[i+1]['step']   # wakeup\n\n                    if start<MAX_SEQ_LEN and end<=MAX_SEQ_LEN: #1\n                        mask1[start:end]=1\n                    elif start<MAX_SEQ_LEN and end>MAX_SEQ_LEN: #2\n                        mask1[start:]=1\n                        mask2[:end-MAX_SEQ_LEN]=1\n                    elif start>=MAX_SEQ_LEN and end<=2*MAX_SEQ_LEN: #3\n                        mask2[start-MAX_SEQ_LEN:end-MAX_SEQ_LEN]=1\n                    elif start>=MAX_SEQ_LEN and end>2*MAX_SEQ_LEN: #4\n                        mask2[start-MAX_SEQ_LEN:]=1\n                        mask3[:end-3*MAX_SEQ_LEN]\n                    elif start>=2*MAX_SEQ_LEN:  #5\n                        mask3[start-2*MAX_SEQ_LEN:end-2*MAX_SEQ_LEN]=1\n            df_to_gt+=[mask1,mask2,mask3]\n\n        elif ID_len_type==3:\n            mask1 = np.zeros(MAX_SEQ_LEN)\n            mask2 = np.zeros(MAX_SEQ_LEN)\n            mask3 = np.zeros(MAX_SEQ_LEN)\n            mask4 = np.zeros(MAX_SEQ_LEN)\n            for i in range(len(labels)):\n                if labels.loc[i]['event']==0:\n                    start=labels.loc[i]['step']   # onset\n                    end=labels.loc[i+1]['step']   # wakeup\n\n                    if start<MAX_SEQ_LEN and end<=MAX_SEQ_LEN: #1\n                        mask1[start:end]=1\n                    elif start<MAX_SEQ_LEN and end>MAX_SEQ_LEN: #2\n                        mask1[start:]=1\n                        mask2[:end-MAX_SEQ_LEN]=1\n                    elif start>=MAX_SEQ_LEN and end<=2*MAX_SEQ_LEN: #3\n                        mask2[start-MAX_SEQ_LEN:end-MAX_SEQ_LEN]=1\n                    elif start>=MAX_SEQ_LEN and end>2*MAX_SEQ_LEN: #4\n                        mask2[start-MAX_SEQ_LEN:]=1\n                        mask3[:end-3*MAX_SEQ_LEN]\n                    elif start>=2*MAX_SEQ_LEN and end<=3*MAX_SEQ_LEN: #5\n                        mask3[start-2*MAX_SEQ_LEN:end-2*MAX_SEQ_LEN]=1\n                    elif start>=2*MAX_SEQ_LEN and end>3*MAX_SEQ_LEN:  #6\n                        mask3[start-2*MAX_SEQ_LEN:]=1\n                        mask4[:end-3*MAX_SEQ_LEN]=1\n                    elif start>=3*MAX_SEQ_LEN:  #7\n                        mask4[start-3*MAX_SEQ_LEN:end-3*MAX_SEQ_LEN]=1\n            df_to_gt+=[mask1,mask2,mask3,mask4]\n\n    return np.array(df_to_gt)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:13:27.517269Z","iopub.execute_input":"2023-10-23T11:13:27.517609Z","iopub.status.idle":"2023-10-23T11:13:27.538377Z","shell.execute_reply.started":"2023-10-23T11:13:27.517572Z","shell.execute_reply":"2023-10-23T11:13:27.537442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = '/kaggle/input/zzz-utime-preprocessing-version1/'","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:13:27.539943Z","iopub.execute_input":"2023-10-23T11:13:27.540287Z","iopub.status.idle":"2023-10-23T11:13:27.550940Z","shell.execute_reply.started":"2023-10-23T11:13:27.540258Z","shell.execute_reply":"2023-10-23T11:13:27.550055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_series = pd.read_parquet(os.path.join(DATA_PATH, 'data/preprocessed/preprocessed_series.parquet')).drop(['step'], axis=1)\nX, X_len_type = series_segmentation_preprocess_cleaning(train_series, clean_type=1)\ndel train_series","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:13:27.551857Z","iopub.execute_input":"2023-10-23T11:13:27.552457Z","iopub.status.idle":"2023-10-23T11:14:10.129989Z","shell.execute_reply.started":"2023-10-23T11:13:27.552427Z","shell.execute_reply":"2023-10-23T11:14:10.129013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_events = pd.read_csv(os.path.join(DATA_PATH, 'data/preprocessed/preprocessed_events.csv'), index_col=0)  # events length min-2 max-70 avg-36\nY_gt = events_segmentation_preprocess_cleaning(train_events, X_len_type, clean_type=1)\ndel train_events","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:14:10.131168Z","iopub.execute_input":"2023-10-23T11:14:10.131461Z","iopub.status.idle":"2023-10-23T11:14:12.450940Z","shell.execute_reply.started":"2023-10-23T11:14:10.131434Z","shell.execute_reply":"2023-10-23T11:14:12.450054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Vaild Split","metadata":{}},{"cell_type":"code","source":"# # Training Data\n# x_train = X[:int(len(X)*(1-VALID_PORTION))]\n# y_train = Y_gt[:int(len(Y_gt)*(1-VALID_PORTION))]\n\n# # Valid Data\n# x_val = X[int(len(X)*(1-VALID_PORTION)):]\n# y_val = Y_gt[int(len(Y_gt)*(1-VALID_PORTION)):]","metadata":{"execution":{"iopub.status.busy":"2023-10-18T18:30:35.826834Z","iopub.execute_input":"2023-10-18T18:30:35.827158Z","iopub.status.idle":"2023-10-18T18:30:35.830432Z","shell.execute_reply.started":"2023-10-18T18:30:35.827137Z","shell.execute_reply":"2023-10-18T18:30:35.829901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train all data!\nx_train = X\ny_train = Y_gt","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:14:12.452162Z","iopub.execute_input":"2023-10-23T11:14:12.452556Z","iopub.status.idle":"2023-10-23T11:14:12.457798Z","shell.execute_reply.started":"2023-10-23T11:14:12.452518Z","shell.execute_reply":"2023-10-23T11:14:12.457064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MODEL","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Input, BatchNormalization, Cropping2D, \\\n                                    Concatenate, MaxPooling2D, Dense, \\\n                                    UpSampling2D, ZeroPadding2D, Lambda, Conv2D, \\\n                                    AveragePooling2D, DepthwiseConv2D","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:14:12.460383Z","iopub.execute_input":"2023-10-23T11:14:12.460885Z","iopub.status.idle":"2023-10-23T11:14:12.469369Z","shell.execute_reply.started":"2023-10-23T11:14:12.460850Z","shell.execute_reply":"2023-10-23T11:14:12.468469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# node1을 node2의 모양과 같게 앞뒤를 '잘라주는' intermediate layer\ndef crop_nodes_to_match(node1, node2, n_crops):\n    # node1: x  --> max_seq_len으로 0패딩한 feature\n    # node2: input\n    s1 = np.array(node1.get_shape().as_list())[1:-2]    # batch빼고, feature_dim(n_classes)랑, 2D계산용 임시차원 빼고\n    s2 = np.array(node2.get_shape().as_list())[1:-2]    # 한마디로 seq\n\n    # 만일 input이랑 모양이 틀리다면...\n    if np.any(s1 != s2):\n        n_crops += 1\n        c = (s1-s2).astype(np.int)      # 유격 얼마나?\n        cr = np.array([c//2, c//2]).flatten()\n        cr[n_crops % 2] += c%2\n        cropped_node1 = Cropping2D([list(cr), [0,0]])(node1)    # layer\n    else:\n        cropped_node1 = node1\n    return cropped_node1, n_crops\n\ndef get_test_model(n_classes, seq_len, channels, depth,\n                   pools, filters, kernel_size, activation, dilation, padding, kernel_reg,\n                   dense_classifier_activation=\"tanh\",\n                   transition_window=1):\n    inputs = Input(shape=(seq_len,channels))\n    reshaped = [-1, seq_len, 1, channels]\n    inp = Lambda(lambda x: tf.reshape(x, reshaped))(inputs)\n    tmp_inp = inp\n\n    \"\"\"\n    1. Encoding path\n    \"\"\"\n    residual_connections = []\n    for i in range(depth):\n        l_name = \"encoder\" + f\"_L{i}\"\n        conv = Conv2D(filters, (kernel_size,1),\n                      activation=activation, padding=padding,\n                      kernel_regularizer=kernel_reg,\n                      dilation_rate=dilation,\n                      name=l_name+\"_conv1\")(inp)\n        bn = BatchNormalization(name=l_name+\"_BN1\")(conv)\n        conv = Conv2D(filters, (kernel_size,1),\n                activation=activation, padding=padding,\n                kernel_regularizer=kernel_reg,\n                dilation_rate=dilation,\n                name=l_name+\"_conv2\")(bn)\n        bn = BatchNormalization(name=l_name+\"_BN2\")(conv)\n        inp = MaxPooling2D(pool_size=(pools[i],1),\n                           name=l_name + \"_pool\")(bn)\n        residual_connections.append(bn)\n        filters = int(filters * 2)\n\n    # Encoding path - Bottom\n    l_name = \"bottom\" + f\"_L{i}\"\n    conv = Conv2D(filters, (kernel_size,1),\n                  activation=activation, padding=padding,\n                  kernel_regularizer=kernel_reg,\n                  dilation_rate=1,\n                  name=l_name+\"_conv1\")(inp)\n    bn = BatchNormalization(name=l_name+\"_BN1\")(conv)\n    conv = Conv2D(filters, (kernel_size, 1),\n                  activation=activation, padding=padding,\n                  kernel_regularizer=kernel_reg,\n                  dilation_rate=1,\n                  name=l_name+\"_conv2\")(bn)\n    x = BatchNormalization(name=l_name+\"_BN2\")(conv)\n\n    \"\"\"\n    2. Decoding path\n    \"\"\"\n    n_crops = 0\n    residual_connections.reverse()\n    for i in range(depth):\n        filters = int(filters/2)\n        l_name = \"decoder\" + f\"_L{i}\"\n\n        # Up-sampling block\n        fs = pools[::-1][i]\n        up = UpSampling2D(size=(fs,1),\n                          name=l_name + \"_up\")(x)\n        conv = Conv2D(filters, (fs,1),\n                      activation=activation,\n                      padding=padding, kernel_regularizer=kernel_reg,\n                      name=l_name + \"_conv1\")(up)\n        bn = BatchNormalization(name=l_name+\"_BN1\")(conv)\n\n        # Crop and concatenate\n        cropped_res,n_crops = crop_nodes_to_match(residual_connections[i], bn, n_crops)\n        # cropped_res = residual_connections[i]\n        merge = Concatenate(axis=-1,\n                            name=l_name + \"_concat\")([cropped_res, bn])\n        conv = Conv2D(filters, (kernel_size, 1),\n                        activation=activation, padding=padding,\n                        kernel_regularizer=kernel_reg,\n                        name=l_name + \"_conv2\")(merge)\n        bn = BatchNormalization(name=l_name + \"_BN2\")(conv)\n        conv = Conv2D(filters, (kernel_size, 1),\n                        activation=activation, padding=padding,\n                        kernel_regularizer=kernel_reg,\n                        name=l_name + \"_conv3\")(bn)\n        x = BatchNormalization(name=l_name + \"_BN3\")(conv)\n\n    \"\"\"\n    3. Dense class modeling\n    \"\"\"\n    x = Conv2D(filters=n_classes,\n                 kernel_size=(1,1),\n                 activation=dense_classifier_activation,    # tanh\n                 name=\"dense_classifier_out\")(x)\n    s = seq_len - x.get_shape().as_list()[1]\n    x = ZeroPadding2D(padding=[[s//2, s//2 + s%2],[0,0]])(x)\n    # feature x에 0을 부족한 만큼 앞뒤에 붙여준다.\n\n    # 처음 input이랑 모양 같도록 앞뒤 잘라준다.\n    x, n_crops = crop_nodes_to_match(\n        node1=x,\n        node2=tmp_inp,\n        n_crops=n_crops\n    )\n\n    \"\"\"\n    Sequence modeling\n    \"\"\"\n    # x = AveragePooling2D((1,1),\n    #                        name=\"average_pool\")(x)\n    x = Conv2D(filters=n_classes,\n                 kernel_size=(transition_window, 1),\n                 activation='sigmoid',  # HERE!!!\n                 kernel_regularizer=regularizers.l2(1e-5),\n                 padding='same',\n                 name='sequence_conv_out')(x)\n    # Conv2D에 sigmoid달면 연산이 어떻게 이뤄지는거지?\n    s = [-1,seq_len,n_classes]\n    out = Lambda(lambda x:tf.reshape(x,s),\n                 name=\"sequence_classification_reshaped\")(x)\n\n    return Model(inputs, out)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:14:12.470919Z","iopub.execute_input":"2023-10-23T11:14:12.471360Z","iopub.status.idle":"2023-10-23T11:14:12.492495Z","shell.execute_reply.started":"2023-10-23T11:14:12.471320Z","shell.execute_reply":"2023-10-23T11:14:12.491456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dice Loss","metadata":{}},{"cell_type":"markdown","source":"from https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch","metadata":{}},{"cell_type":"code","source":"import numpy\nimport tensorflow as tf\nimport keras\nimport keras.backend as K","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:14:12.494178Z","iopub.execute_input":"2023-10-23T11:14:12.494988Z","iopub.status.idle":"2023-10-23T11:14:12.507760Z","shell.execute_reply.started":"2023-10-23T11:14:12.494953Z","shell.execute_reply":"2023-10-23T11:14:12.506733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_loss(y_true, y_pred, smooth=1e-6):\n    intersection = tf.math.reduce_sum(tf.math.multiply(y_true,y_pred))\n    dice = (2*intersection + smooth) / (tf.math.reduce_sum(y_true)+tf.math.reduce_sum(y_pred)+smooth)\n    return 1 - dice\n# K.sum() = tf.math.reduce_sum() [sum over all dimensions.]\n# K.dot() = tf.math.multiply(tf.math.reduce_sum())","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:14:12.509236Z","iopub.execute_input":"2023-10-23T11:14:12.509545Z","iopub.status.idle":"2023-10-23T11:14:12.519648Z","shell.execute_reply.started":"2023-10-23T11:14:12.509516Z","shell.execute_reply":"2023-10-23T11:14:12.518652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRAINING","metadata":{}},{"cell_type":"code","source":"# model\nmodel = get_test_model(n_classes=N_CLASSES,\n                       seq_len=MAX_SEQ_LEN,\n                       channels=N_CHANNELS,\n                       depth=4,\n                       pools=(10,8,6,4),\n                       filters=int(8*2),\n                       kernel_size=5,\n                       activation='elu',\n                       dilation=2,\n                       padding=\"same\",\n                       kernel_reg=None)\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n              loss=dice_loss,\n              metrics=[tf.keras.metrics.Recall(thresholds=THRESHOLD),\n                       tf.keras.metrics.Precision(thresholds=THRESHOLD),\n                       tf.keras.metrics.BinaryAccuracy(threshold=THRESHOLD)\n                    ],\n)\n\n# log_dir = OUT_PATH + 'logs/fit/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n#                                                       histogram_freq=1)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:15:15.544605Z","iopub.execute_input":"2023-10-23T11:15:15.545395Z","iopub.status.idle":"2023-10-23T11:15:16.416634Z","shell.execute_reply.started":"2023-10-23T11:15:15.545360Z","shell.execute_reply":"2023-10-23T11:15:16.415741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN!\nmodel.fit(x=x_train,\n          y=y_train,\n          batch_size=BATCH_SIZE,\n          epochs=EPOCHS,\n          verbose=2,\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:15:18.440525Z","iopub.execute_input":"2023-10-23T11:15:18.441451Z","iopub.status.idle":"2023-10-23T11:15:56.093663Z","shell.execute_reply.started":"2023-10-23T11:15:18.441416Z","shell.execute_reply":"2023-10-23T11:15:56.092064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Temporary\n# CKPT_DIR_PATH = os.path.join(OUT_PATH, 'models/ckpt')\n\n# model.save_weights(CKPT_DIR_PATH+'/baseline_bs1_epoch50')","metadata":{"execution":{"iopub.status.busy":"2023-10-12T04:42:00.944136Z","iopub.execute_input":"2023-10-12T04:42:00.944752Z","iopub.status.idle":"2023-10-12T04:42:01.378149Z","shell.execute_reply.started":"2023-10-12T04:42:00.944718Z","shell.execute_reply":"2023-10-12T04:42:01.377178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# loss, acc = model.evaluate(x_val, y_val, verbose=2)\n# print(f'acc: {acc}')\n# print(f'loss: {loss}')","metadata":{"execution":{"iopub.status.busy":"2023-10-18T12:10:32.656194Z","iopub.execute_input":"2023-10-18T12:10:32.656541Z","iopub.status.idle":"2023-10-18T12:10:35.101948Z","shell.execute_reply.started":"2023-10-18T12:10:32.656512Z","shell.execute_reply":"2023-10-18T12:10:35.101209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"plot some loss and acc here, diverse metrics","metadata":{}},{"cell_type":"code","source":"# idx = 10\n\n# sample_x = tf.reshape(x_val[idx],(1,-1,2))\n# sample_y = y_val[idx]\n\n# loc=[]\n# for i in range(len(sample_y)-1):\n#     if sample_y[i]==0 and sample_y[i+1]==1:\n#         loc.append(i+1)\n# print(len(loc))\n\n# sample_pred = model.predict(sample_x)\n# sample_pred = sample_pred.reshape(-1)\n\n# for i in loc:\n#     print(sample_pred[i-100:i])\n#     break","metadata":{"execution":{"iopub.status.busy":"2023-10-18T13:07:56.571079Z","iopub.execute_input":"2023-10-18T13:07:56.571606Z","iopub.status.idle":"2023-10-18T13:07:56.811632Z","shell.execute_reply.started":"2023-10-18T13:07:56.571576Z","shell.execute_reply":"2023-10-18T13:07:56.810822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X, Y_gt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's submit this lame DL approach.","metadata":{}},{"cell_type":"code","source":"# Prediction threshold ! over this will be considered as sleeping steps.\n# it is equal to model's threshold.(maybe, need to be checked)\nTHRESHOLD = 0.5","metadata":{"execution":{"iopub.status.busy":"2023-10-18T14:40:44.669490Z","iopub.execute_input":"2023-10-18T14:40:44.670067Z","iopub.status.idle":"2023-10-18T14:40:44.673375Z","shell.execute_reply.started":"2023-10-18T14:40:44.670037Z","shell.execute_reply":"2023-10-18T14:40:44.672601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Premise: y_true is one hot vector\ndef series_segmentation_preprocess_test(series):\n    \"\"\"\n    INPUTS\n        - series: preprocessed series\n\n    OUTPUTS\n        - X: (N, MAX_SEQ_LEN, 2) truncated, padded series\n    \"\"\"\n\n    # for my preprocessed data only\n    X = []\n    X_len_type=[]\n    # how many people?\n    total_id_num = int(series['id_index'].iloc[-1]) + 1\n\n    # make them into numpy for training\n    for i in range(total_id_num):\n        series_per_id = series.loc[series['id_index'] == i].drop(['id_index'], axis=1)\n        # crop series longer than MAX_SEQ_LEN for the model!\n        if len(series_per_id) <= MAX_SEQ_LEN:\n            seq1 = series_per_id.to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq1)\n            seq1 = np.pad(seq1, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            X += [seq1]\n            len_type=0\n            X_len_type.append(len_type)\n        elif MAX_SEQ_LEN < len(series_per_id) <= 2*MAX_SEQ_LEN:\n            seq1 = series_per_id[:MAX_SEQ_LEN].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq1)\n            seq1 = np.pad(seq1, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            seq2 = series_per_id[MAX_SEQ_LEN:].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq2)\n            seq2 = np.pad(seq2, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            X += [seq1,seq2]\n            len_type=1\n            X_len_type.append(len_type)\n        elif 2*MAX_SEQ_LEN < len(series_per_id) <= 3*MAX_SEQ_LEN:\n            seq1 = series_per_id[:MAX_SEQ_LEN].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq1)\n            seq1=np.pad(seq1, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            seq2 = series_per_id[MAX_SEQ_LEN:2*MAX_SEQ_LEN].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq2)\n            seq2=np.pad(seq2, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            seq3 = series_per_id[2*MAX_SEQ_LEN:].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq3)\n            seq3=np.pad(seq3, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            X += [seq1,seq2,seq3]\n            len_type=2\n            X_len_type.append(len_type)\n        elif 3*MAX_SEQ_LEN < len(series_per_id) <= 4*MAX_SEQ_LEN:\n            seq1 = series_per_id[:MAX_SEQ_LEN].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq1)\n            seq1 = np.pad(seq1, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            seq2 = series_per_id[MAX_SEQ_LEN:2*MAX_SEQ_LEN].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq2)\n            seq2 = np.pad(seq2, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            seq3 = series_per_id[2*MAX_SEQ_LEN:3*MAX_SEQ_LEN].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq3)\n            seq3 = np.pad(seq3, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            seq4 = series_per_id[3*MAX_SEQ_LEN:].to_numpy()\n            pad_amount = MAX_SEQ_LEN - len(seq4)\n            seq4 = np.pad(seq4, ((0,pad_amount),(0,0)), 'constant', constant_values=0)\n            X += [seq1,seq2,seq3,seq4]\n            len_type=3\n            X_len_type.append(len_type)\n        else:\n            assert False, \"MAX_SEQ_LEN > 1,840,000 is yet implemented for this dataset. something you are doing wrong.\"\n\n    return np.array(X), np.array(X_len_type)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test data processing\nseries = pd.read_parquet('/kaggle/input/child-mind-institute-detect-sleep-states/test_series.parquet').drop(['timestamp','step'], axis=1)\n\nid_map = pd.DataFrame({'series_id':series.series_id.unique(),\n                       'id_index': [i for i in range(len(series.series_id.unique()))]})\nid_map.id_index = id_map.id_index.astype(np.uint16)\nseries = series.merge(right=id_map, on='series_id').drop(columns='series_id')\n\n# Normalize anglez, enmo\nmean_enmo = 0.041315034\nstd_enmo = 0.09743800759315491\nseries['enmo'] = (series['enmo'] - mean_enmo)/std_enmo\nmean_anglez = -8.810453\nstd_anglez = 30.157093048095703\nseries['anglez'] = (series['anglez'] - mean_anglez)/std_anglez","metadata":{"execution":{"iopub.status.busy":"2023-10-18T14:40:47.641120Z","iopub.execute_input":"2023-10-18T14:40:47.641696Z","iopub.status.idle":"2023-10-18T14:40:47.869146Z","shell.execute_reply.started":"2023-10-18T14:40:47.641669Z","shell.execute_reply":"2023-10-18T14:40:47.868457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def index_to_id(ID_int, id_map):\n    ID_str = id_map.iloc[ID_int][\"series_id\"]\n    return ID_str","metadata":{"execution":{"iopub.status.busy":"2023-10-18T14:40:58.887498Z","iopub.execute_input":"2023-10-18T14:40:58.887883Z","iopub.status.idle":"2023-10-18T14:40:58.892092Z","shell.execute_reply.started":"2023-10-18T14:40:58.887857Z","shell.execute_reply":"2023-10-18T14:40:58.891258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Naive Prediction\ntest_inputs, test_len_types = series_segmentation_preprocess_test(series)\ntest_lens = np.add(test_len_types, 1)\ndel series\n\nsub_ids=[]\nsub_steps=[]\nsub_events=[]\nsub_scores=[]\n\nfor ID, test_len in enumerate(test_lens):\n    # make prediction\n    start_i = np.sum(test_lens[:ID])\n    end_i = start_i + test_len\n    preds=[]\n    for i in range(start_i,end_i):\n        input = test_inputs[i][np.newaxis,:]\n        output = model.predict(input).reshape(-1)\n        preds.append(output)\n    pred = np.concatenate(preds,axis=0)\n    \n    # make submission from predictioion\n    sleeping_flag=False\n    for j in range(len(pred)):\n        # Inference algorithm here !\n        if pred[j]>THRESHOLD and not sleeping_flag:\n            onset=j\n            sub_ids.append(index_to_id(ID,id_map))\n            sub_steps.append(onset)\n            sub_events.append('onset')\n            sub_scores.append(pred[onset])\n            sleeping_flag=True\n        if pred[j]<THRESHOLD and sleeping_flag:\n            wakeup=j\n            sub_ids.append(index_to_id(ID,id_map))\n            sub_steps.append(wakeup)\n            sub_events.append('wakeup')\n            sub_scores.append(pred[wakeup])\n            sleeping_flag=False\n\nsubmission = pd.DataFrame(data={'series_id':sub_ids, 'step':sub_steps, 'event':sub_events, 'score':sub_scores})\nsub_row_id = [i for i in range(len(submission))]\nsubmission.insert(0, \"row_id\", sub_row_id)\nsubmission.to_csv('submission.csv', index=False)\n# display(submission)","metadata":{"execution":{"iopub.status.busy":"2023-10-18T15:03:22.136929Z","iopub.execute_input":"2023-10-18T15:03:22.137465Z","iopub.status.idle":"2023-10-18T15:03:22.149558Z","shell.execute_reply.started":"2023-10-18T15:03:22.137439Z","shell.execute_reply":"2023-10-18T15:03:22.148827Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
