{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":53666,"databundleVersionId":6589269,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Credits:\n\n- https://www.kaggle.com/code/itsuki9180/detect-sleep-states-dataprepare  \n- https://www.kaggle.com/code/werus23/sleep-critical-point-prepare-data\n\nGO TO:\nData Preparation - this\nTRAIN - https://www.kaggle.com/code/jhjh97/transformer-train\nINFERENCE - Not Yet\n\nThis does:\n- Save preprocessed gaussian outputs according to following parameters `N_DAYS_PER_SAMPLE`, `SEPERATE_FEATURES`.\n- `N_DAYS_PER_SAMPLE`(int) is how many days you like to windowing for model.\n- `SEPERATE_FEATURES`(bool) whether you split your data for seperated model.\n- `SIGMA` for gaussian sigma value(std). more smaller SIGMA is, more sharper distribution gets.","metadata":{}},{"cell_type":"markdown","source":"## Ver1.1","metadata":{}},{"cell_type":"markdown","source":"- Split hard sample and easy samples","metadata":{}},{"cell_type":"code","source":"N_DAYS_PER_SAMPLE = 3\nFIT_INTO_1DAY = True\nSEPERATE_FEATURES = True  # Not Used\n# GENERATE_FEATURES = True # generate in Train torch.data.DataSet for efficiency\n\nSIGMA = 720 # target gaussian parameter, 12 * 60","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-10T10:52:29.179056Z","iopub.execute_input":"2023-11-10T10:52:29.179509Z","iopub.status.idle":"2023-11-10T10:52:29.222094Z","shell.execute_reply.started":"2023-11-10T10:52:29.179465Z","shell.execute_reply":"2023-11-10T10:52:29.220669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nimport time\nimport json\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport os\nfrom os.path import join, exists\nimport joblib\nimport random\nimport math\nfrom tqdm.auto import tqdm \nimport shutil\n\nfrom scipy.interpolate import interp1d\n\nfrom math import pi, sqrt, exp\nimport sklearn,sklearn.model_selection\nimport torch\nfrom torch import nn,Tensor\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\nfrom sklearn.metrics import average_precision_score\nfrom timm.scheduler import CosineLRScheduler\nplt.style.use(\"ggplot\")\n\nfrom pyarrow.parquet import ParquetFile\nimport pyarrow as pa \nimport ctypes","metadata":{"execution":{"iopub.status.busy":"2023-11-10T10:52:30.476046Z","iopub.execute_input":"2023-11-10T10:52:30.476451Z","iopub.status.idle":"2023-11-10T10:52:37.957730Z","shell.execute_reply.started":"2023-11-10T10:52:30.476420Z","shell.execute_reply":"2023-11-10T10:52:37.956409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_dir = \"train_data\"\nos.makedirs(out_dir, exist_ok = True)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T10:52:37.959808Z","iopub.execute_input":"2023-11-10T10:52:37.960139Z","iopub.status.idle":"2023-11-10T10:52:37.965673Z","shell.execute_reply.started":"2023-11-10T10:52:37.960110Z","shell.execute_reply":"2023-11-10T10:52:37.964385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PATHS:\n    MAIN_DIR = \"/kaggle/input/child-mind-institute-detect-sleep-states/\"\n    # CSV FILES : \n    TRAIN_EVENTS = MAIN_DIR + \"train_events.csv\"\n    # PARQUET FILES:\n    TRAIN_SERIES = MAIN_DIR + \"train_series.parquet\"\n    SAVE_DIR = out_dir","metadata":{"execution":{"iopub.status.busy":"2023-11-10T10:52:37.967308Z","iopub.execute_input":"2023-11-10T10:52:37.968049Z","iopub.status.idle":"2023-11-10T10:52:37.981314Z","shell.execute_reply.started":"2023-11-10T10:52:37.968007Z","shell.execute_reply":"2023-11-10T10:52:37.980115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_events(data_path, save_dir):\n    \"\"\" events.csv \"\"\"\n    events = pd.read_csv(data_path)\n    if not exists(join(save_dir, 'id_map.parquet')):\n        id_map = pd.DataFrame({\"series_id\": events.series_id.unique(),\n                                \"id_index\": [i for i in range(len(events.series_id.unique()))]})\n        id_map.id_index = id_map.id_index.astype(np.uint16)\n        id_map.to_parquet(os.path.join(save_dir,\"id_map.parquet\"), index=False)\n    id_map = pd.read_parquet(os.path.join(save_dir,\"id_map.parquet\"))\n    events.dropna(subset=['step'], inplace=True)\n    # reduce memory size + event re-label\n    events = events.merge(right=id_map, on=\"series_id\").drop(columns=\"series_id\")\n    events.step = events.step.astype(np.uint32)\n    events.night = events.night.astype(np.uint16)\n#     events.to_csv(os.path.join(save_dir,'preprocessed_events.csv'), index=False)\n    \n    return events","metadata":{"execution":{"iopub.status.busy":"2023-11-10T10:52:37.984996Z","iopub.execute_input":"2023-11-10T10:52:37.985480Z","iopub.status.idle":"2023-11-10T10:52:37.999688Z","shell.execute_reply.started":"2023-11-10T10:52:37.985437Z","shell.execute_reply":"2023-11-10T10:52:37.998136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_series(data_path, save_dir):\n    \"\"\" train_series.parquet \"\"\"\n    # reduce memory\n    series = pd.read_parquet(data_path)\n    if not exists(join(save_dir, 'id_map.parquet')):\n        id_map = pd.DataFrame({\"series_id\": series.series_id.unique(),\n                                \"id_index\": [i for i in range(len(series.series_id.unique()))]})\n        id_map.id_index = id_map.id_index.astype(np.uint16)\n        id_map.to_parquet(os.path.join(save_dir,\"id_map.parquet\"), index=False)\n    \n    id_map = pd.read_parquet(os.path.join(save_dir,\"id_map.parquet\"))\n    series = series.merge(right=id_map, on='series_id').drop(columns='series_id').reset_index()\n    series.step = series.step.astype(np.uint32)\n\n    # normalize enmo and anglez\n    mean_enmo = series['enmo'].mean()\n    std_enmo = series['enmo'].std()\n    series.enmo = (series['enmo'] - mean_enmo)/std_enmo\n    mean_anglez = series['anglez'].mean()\n    std_anglez = series['anglez'].std()\n    series.anglez = (series['anglez'] - mean_anglez)/std_anglez\n    \n    series = series.drop(columns='index')\n#     series.to_parquet(os.path.join(save_dir,'preprocessed_series.parquet'))\n    \n    return series","metadata":{"execution":{"iopub.status.busy":"2023-11-10T10:52:38.001034Z","iopub.execute_input":"2023-11-10T10:52:38.001464Z","iopub.status.idle":"2023-11-10T10:52:38.020376Z","shell.execute_reply.started":"2023-11-10T10:52:38.001433Z","shell.execute_reply":"2023-11-10T10:52:38.018945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# must process events first.\nevents = preprocess_events(PATHS.TRAIN_EVENTS, PATHS.SAVE_DIR)\nseries = preprocess_series(PATHS.TRAIN_SERIES, PATHS.SAVE_DIR)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T10:52:38.021768Z","iopub.execute_input":"2023-11-10T10:52:38.022135Z","iopub.status.idle":"2023-11-10T10:54:43.722469Z","shell.execute_reply.started":"2023-11-10T10:52:38.022095Z","shell.execute_reply":"2023-11-10T10:54:43.720352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"converting evetns into gaussian target numpy array.","metadata":{}},{"cell_type":"code","source":"def gauss(n=SIGMA,sigma=SIGMA*0.15):\n    # guassian distribution function\n    r = range(-int(n/2),int(n/2)+1)  # int는 나머지 버림계산\n    return [1 / (sigma * sqrt(2*pi)) * exp(-float(x)**2/(2*sigma**2)) for x in r]","metadata":{"execution":{"iopub.status.busy":"2023-11-10T10:54:43.724759Z","iopub.execute_input":"2023-11-10T10:54:43.725283Z","iopub.status.idle":"2023-11-10T10:54:43.734785Z","shell.execute_reply.started":"2023-11-10T10:54:43.725238Z","shell.execute_reply":"2023-11-10T10:54:43.733522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_pad_amount(series):\n    # For fitting data into 24 hours cycle, get how much series need to be zero_padded for front and end.\n    # 0.4145 secs\n    f_h=series.timestamp.dt.hour[0]\n    f_m=series.timestamp.dt.minute[0]\n    f_s=series.timestamp.dt.second[0]\n    r_h=list(series.timestamp.dt.hour)[-1]\n    r_m=list(series.timestamp.dt.minute)[-1]\n    r_s=list(series.timestamp.dt.second)[-1]\n    \n    # calc how many steps first data is apart from 0:00:00.\n    front = f_s//5 + f_m*12 + f_h*(12*60)\n    if front>0:\n        front -= 1\n    \n    # calc how many steps rear data is apart from 23:59:55\n    max_step = 24*60*12\n    end = max_step - (r_s//5 + r_m*12 + r_h*(12*60))\n    \n    return front, end","metadata":{"execution":{"iopub.status.busy":"2023-11-10T10:54:43.736524Z","iopub.execute_input":"2023-11-10T10:54:43.737408Z","iopub.status.idle":"2023-11-10T10:54:43.751763Z","shell.execute_reply.started":"2023-11-10T10:54:43.737363Z","shell.execute_reply":"2023-11-10T10:54:43.750555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# New 2","metadata":{}},{"cell_type":"markdown","source":"### can i reduce data size? -> Y\n\n- 어차피 conv 계열만 쓰기에, X 임베딩을 0-24로 맞출 필요가 없음.\n- 하지만, 잠들기 전 후로 데이터에 증거가 많이 남아있을 것 같음. 그래서 (전날 오후 6시 ~ 당일 ~ 다음날 오전 6시)로 시간을 맞춰줄 예정. \n- 24+6+6=36시간 = 25920, /60=432  \n\n#### easy, hard(unlabelded)로 나눠주기\n\n- 일단 easy로 학습시키고 모델이 얼마나 잘 성능 나오는지 보고싶음\n- easy(train,val), mixed(train,val), hard(tarin,val)\n- 기준은 하루동안 label이 없으면 hard example","metadata":{}},{"cell_type":"code","source":"data_counter=0\nenmo_x=[]\nenmo_y=[]\nattach_hours=6\nids = pd.read_parquet(os.path.join(PATHS.SAVE_DIR,\"id_map.parquet\"))\n\nif SEPERATE_FEATURES:\n    os.makedirs(PATHS.SAVE_DIR+'/enmo/hard', exist_ok = True)\n    os.makedirs(PATHS.SAVE_DIR+'/enmo/fine', exist_ok = True)\n    os.makedirs(PATHS.SAVE_DIR+'/anglez/hard', exist_ok = True)\n    os.makedirs(PATHS.SAVE_DIR+'/anglez/fine', exist_ok = True)\n\n# for ids, make \"series + target_gaussian\" vector and store them ids by ids.\nfor cur_id in tqdm(ids.id_index, total=len(ids)):\n    cur_targets = []\n    cur_events = events[events.id_index == cur_id].copy()\n    cur_series = series.loc[(series.id_index == cur_id)].copy().reset_index(drop=True)\n    cur_series['timestamp'] = pd.to_datetime(cur_series.timestamp,format = '%Y-%m-%dT%H:%M:%S%z').astype(\"datetime64[ns, UTC-04:00]\")\n    \n    # calculate how many steps needed to pad for each series\n    front, end = get_pad_amount(cur_series)\n    cur_events.step += front\n    total_len = len(cur_series)+front+end\n    target_gaussian = np.zeros((total_len,2))  # onset:0, wakeup:1\n    for i in range(len(cur_events)):\n        s=cur_events.iloc[i].step\n        gau_st = max(SIGMA//2-s, 0)\n        gau_ed = SIGMA+(SIGMA+1)%2+1 - max(s+SIGMA//2-(len(target_gaussian)-1), 0)\n        tar_st = max(0, s-SIGMA//2)\n        tar_ed = min(len(target_gaussian), s+SIGMA//2+1)\n        if cur_events.iloc[i].event=='onset':\n            target_gaussian[tar_st:tar_ed,0] = gauss()[gau_st:gau_ed]\n        if cur_events.iloc[i].event=='wakeup':\n            target_gaussian[tar_st:tar_ed,1] = gauss()[gau_st:gau_ed]\n\n    # zero padding for series\n    cur_series = cur_series.drop(columns=['id_index','step','timestamp'])\n    zero_pad_front = pd.DataFrame({'anglez':[0 for _ in range(front)], 'enmo':[0 for _ in range(front)]}, dtype=np.float32)\n    zero_pad_end = pd.DataFrame({'anglez':[0 for _ in range(end)], 'enmo':[0 for _ in range(end)]}, dtype=np.float32)\n    cur_series = pd.concat([zero_pad_front, cur_series, zero_pad_end], ignore_index=True)\n    del zero_pad_front, zero_pad_end\n    gc.collect()\n    \n    # normalize target into max value.\n    target_gaussian /= np.max(target_gaussian + 1e-12)\n    \n    # append target onto series,\n    cur_series['onset'] = target_gaussian[:,0]\n    cur_series['wakeup'] = target_gaussian[:,1]\n    \n    # save it into croped size\n    one_day_steps = (24*60*12)\n    cur_series_days = len(cur_series)//one_day_steps\n    for i in range(cur_series_days - N_DAYS_PER_SAMPLE + 1):\n        st = i*one_day_steps\n        ed = (i+N_DAYS_PER_SAMPLE)*one_day_steps\n        st2=(24-attach_hours)*60*12\n        ed2=-(24-attach_hours)*60*12\n        enmo_sample = cur_series[st:ed].drop(columns='anglez')[st2:ed2]\n        \n        if sum(enmo_sample['onset'])==0.0 and sum(enmo_sample['wakeup'])==0.0:\n            # label없는 seq는 hard에 저장\n            # plotting(enmo_sample)\n            enmo_sample.to_csv(f'{PATHS.SAVE_DIR}/enmo/hard/enmo_id{cur_id}_{data_counter}.csv', index=False)\n        else:\n            # onset, wakeup 하나라도 있으면 fine에 저장\n            enmo_sample.to_csv(f'{PATHS.SAVE_DIR}/enmo/fine/enmo_id{cur_id}_{data_counter}.csv', index=False)\n        anglez_sample = cur_series[st:ed].drop(columns='enmo')\n        if sum(anglez_sample['onset'])==0.0 and sum(anglez_sample['wakeup']==0.0):\n            anglez_sample.to_csv(f'{PATHS.SAVE_DIR}/anglez/hard/anglez_id{cur_id}_{data_counter}.csv', index=False)\n        else:\n            anglez_sample.to_csv(f'{PATHS.SAVE_DIR}/anglez/fine/anglez_id{cur_id}_{data_counter}.csv', index=False)\n        data_counter+=1\n    del cur_series, cur_events, target_gaussian, enmo_sample, anglez_sample\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-11-10T10:54:43.753473Z","iopub.execute_input":"2023-11-10T10:54:43.753957Z","iopub.status.idle":"2023-11-10T10:57:06.132686Z","shell.execute_reply.started":"2023-11-10T10:54:43.753921Z","shell.execute_reply":"2023-11-10T10:57:06.130509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NEW\n\n- including 8 hours only front, back\n- feature engineering included\n- gaussian label\n- directly store 28GB data\n\n\n#### ISSUE\n\n- \"failed\" because of memory shortage","metadata":{}},{"cell_type":"code","source":"# def generate_features_enmo(xy):\n#     for r in [17, 33, 65]:\n#         tmp_feat = xy['enmo'].rolling(r, center=True)\n#         xy[f'enmo_mean_{r}'] = tmp_feat.mean()\n#         xy[f'enmo_std_{r}'] = tmp_feat.std()\n#     return xy.drop(columns=['onset','wakeup']).fillna(0)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T10:57:06.134041Z","iopub.status.idle":"2023-11-10T10:57:06.134679Z","shell.execute_reply.started":"2023-11-10T10:57:06.134340Z","shell.execute_reply":"2023-11-10T10:57:06.134372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_counter=0\n# enmo_x=[]\n# enmo_y=[]\n# ids = pd.read_parquet(os.path.join(PATHS.SAVE_DIR,\"id_map.parquet\"))\n\n# if SEPERATE_FEATURES:\n#     os.makedirs(PATHS.SAVE_DIR+'/enmo', exist_ok = True)\n#     os.makedirs(PATHS.SAVE_DIR+'/anglez', exist_ok = True)\n\n    \n# # for ids, make \"series + target_gaussian\" vector and store them ids by ids.\n# for idx, cur_id in tqdm(enumerate(ids.id_index), total=len(ids)):\n#     cur_targets = []\n#     cur_events = events[events.id_index == cur_id].copy()\n#     cur_series = series.loc[(series.id_index == cur_id)].copy().reset_index(drop=True)\n#     cur_series['timestamp'] = pd.to_datetime(cur_series.timestamp,format = '%Y-%m-%dT%H:%M:%S%z').astype(\"datetime64[ns, UTC-04:00]\")\n    \n#     # calculate how many steps needed to pad for each series\n#     front, end = get_pad_amount(cur_series)\n#     cur_events.step += front\n#     total_len = len(cur_series)+front+end\n#     target_gaussian = np.zeros((total_len,2))  # onset:0, wakeup:1\n    \n#     for i in range(len(cur_events)):\n#         s=cur_events.iloc[i].step\n#         gau_st = max(SIGMA//2-s, 0)\n#         gau_ed = SIGMA+(SIGMA+1)%2+1 - max(s+SIGMA//2-(len(target_gaussian)-1), 0)\n#         tar_st = max(0, s-SIGMA//2)\n#         tar_ed = min(len(target_gaussian), s+SIGMA//2+1)\n#         if cur_events.iloc[i].event=='onset':\n#             target_gaussian[tar_st:tar_ed,0] = gauss()[gau_st:gau_ed]\n#         if cur_events.iloc[i].event=='wakeup':\n#             target_gaussian[tar_st:tar_ed,1] = gauss()[gau_st:gau_ed]\n    \n#     print(cur_series)\n#     break\n#     # zero padding for series\n#     cur_series = cur_series.drop(columns=['id_index','step','timestamp'])\n#     zero_pad_front = pd.DataFrame({'anglez':[0 for _ in range(front)], 'enmo':[0 for _ in range(front)]}, dtype=np.float32)\n#     zero_pad_end = pd.DataFrame({'anglez':[0 for _ in range(end)], 'enmo':[0 for _ in range(end)]}, dtype=np.float32)\n#     cur_series = pd.concat([zero_pad_front, cur_series, zero_pad_end], ignore_index=True)\n#     del zero_pad_front, zero_pad_end\n#     gc.collect()\n    \n#     # normalize target into max value.\n#     target_gaussian /= np.max(target_gaussian + 1e-12)\n    \n#     # append target onto series, save it.\n#     cur_series['onset'] = target_gaussian[:,0]\n#     cur_series['wakeup'] = target_gaussian[:,1]\n    \n#     one_day_steps = (24*60*12)\n#     cur_series_days = len(cur_series)//one_day_steps\n#     for i in range(cur_series_days - N_DAYS_PER_SAMPLE + 1):\n#         st = i*one_day_steps\n#         ed = (i+N_DAYS_PER_SAMPLE)*one_day_steps\n#         if SEPERATE_FEATURES:\n#             st2=(24-attach_hours)*60*12\n#             ed2=-(24-attach_hours)*60*12\n#             anglez.append(cur_series[st:ed].drop(columns='enmo')[st2:ed2].to_numpy())\n#             enmo_xy = cur_series[st:ed].drop(columns='anglez')\n#             x = generate_features_enmo(enmo_xy).to_numpy()[st2:ed2]\n#             y = enmo_xy[['onset','wakeup']].to_numpy()[st2:ed2]\n#             enmo_x.append(x)\n#             enmo_y.append(y)\n#         else:\n#             pass\n#         data_counter+=1\n        \n#     del cur_series, cur_events, target_gaussian\n#     gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-11-10T05:59:40.505532Z","iopub.execute_input":"2023-11-10T05:59:40.505851Z","iopub.status.idle":"2023-11-10T05:59:49.577584Z","shell.execute_reply.started":"2023-11-10T05:59:40.505824Z","shell.execute_reply":"2023-11-10T05:59:49.576190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PREV","metadata":{}},{"cell_type":"code","source":"# targets = []\n# data = []\n# data_counter=0\n# ids = pd.read_parquet(os.path.join(PATHS.SAVE_DIR,\"id_map.parquet\"))\n\n# if SEPERATE_FEATURES:\n#     os.makedirs(PATHS.SAVE_DIR+'/enmo', exist_ok = True)\n#     os.makedirs(PATHS.SAVE_DIR+'/anglez', exist_ok = True)\n\n# # for ids, make \"series + target_gaussian\" vector and store them ids by ids.\n# for cur_id in tqdm(ids.id_index, total=len(ids)):\n#     cur_targets = []\n#     cur_events = events[events.id_index == cur_id].copy()\n#     cur_series = series.loc[(series.id_index == cur_id)].copy().reset_index(drop=True)\n#     cur_series['timestamp'] = pd.to_datetime(cur_series.timestamp,format = '%Y-%m-%dT%H:%M:%S%z').astype(\"datetime64[ns, UTC-04:00]\")\n    \n#     # calculate how many steps needed to pad for each series\n#     front, end = get_pad_amount(cur_series)\n#     cur_events.step += front\n#     total_len = len(cur_series)+front+end\n#     target_gaussian = np.zeros((total_len,2))  # onset:0, wakeup:1\n#     for i in range(len(cur_events)):\n#         s=cur_events.iloc[i].step\n#         gau_st = max(SIGMA//2-s, 0)\n#         gau_ed = SIGMA+(SIGMA+1)%2+1 - max(s+SIGMA//2-(len(target_gaussian)-1), 0)\n#         tar_st = max(0, s-SIGMA//2)\n#         tar_ed = min(len(target_gaussian), s+SIGMA//2+1)\n#         if cur_events.iloc[i].event=='onset':\n#             target_gaussian[tar_st:tar_ed,0] = gauss()[gau_st:gau_ed]\n#         if cur_events.iloc[i].event=='wakeup':\n#             target_gaussian[tar_st:tar_ed,1] = gauss()[gau_st:gau_ed]\n\n#     # zero padding for series\n#     cur_series = cur_series.drop(columns=['id_index','step','timestamp'])\n#     zero_pad_front = pd.DataFrame({'anglez':[0 for _ in range(front)], 'enmo':[0 for _ in range(front)]}, dtype=np.float32)\n#     zero_pad_end = pd.DataFrame({'anglez':[0 for _ in range(end)], 'enmo':[0 for _ in range(end)]}, dtype=np.float32)\n#     cur_series = pd.concat([zero_pad_front, cur_series, zero_pad_end], ignore_index=True)\n#     del zero_pad_front, zero_pad_end\n#     gc.collect()\n    \n#     # normalize target into max value.\n#     target_gaussian /= np.max(target_gaussian + 1e-12)\n    \n#     # append target onto series, save it.\n#     cur_series['onset'] = target_gaussian[:,0]\n#     cur_series['wakeup'] = target_gaussian[:,1]\n    \n#     one_day_steps = (24*60*12)\n#     cur_series_days = len(cur_series)//one_day_steps\n#     for i in range(cur_series_days - N_DAYS_PER_SAMPLE + 1):\n#         st = i*one_day_steps\n#         ed = (i+N_DAYS_PER_SAMPLE)*one_day_steps\n#         if SEPERATE_FEATURES:\n#             cur_series[st:ed].drop(columns='anglez').copy().to_csv(f'{PATHS.SAVE_DIR}/enmo/enmo_id{cur_id}_{data_counter}.csv', index=False)\n#             cur_series[st:ed].drop(columns='enmo').copy().to_csv(f'{PATHS.SAVE_DIR}/anglez/anglez_id{cur_id}_{data_counter}.csv', index=False)\n#         else:\n#             pass\n#             # cur_series[st:ed].copy().to_csv(f'{PATHS.SAVE_DIR}/id{cur_id}_total{data_counter}.csv', index=False)\n#         data_counter+=1\n        \n#     del cur_series, cur_events, target_gaussian\n#     gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## dir move","metadata":{}},{"cell_type":"code","source":"# Zip file. Because amount of data is over 50.\nshutil.move(f'{PATHS.SAVE_DIR}/id_map.parquet', './')\nevents[:3].copy().to_csv('./dummy.csv')\n\nif SEPERATE_FEATURES:\n    os.makedirs('./enmo', exist_ok = True)\n    os.makedirs('./anglez', exist_ok = True)\n    shutil.make_archive('./enmo', 'zip', f'{PATHS.SAVE_DIR}/enmo')\n    shutil.make_archive('./anglez', 'zip', f'{PATHS.SAVE_DIR}/anglez')\n    shutil.rmtree(f'{PATHS.SAVE_DIR}')\n    shutil.rmtree(f'./enmo')\n    shutil.rmtree(f'./anglez')\nelse:\n    pass","metadata":{"execution":{"iopub.status.busy":"2023-11-10T06:42:27.072355Z","iopub.execute_input":"2023-11-10T06:42:27.072829Z","iopub.status.idle":"2023-11-10T06:42:49.753584Z","shell.execute_reply.started":"2023-11-10T06:42:27.072794Z","shell.execute_reply":"2023-11-10T06:42:49.752289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# check","metadata":{}},{"cell_type":"code","source":"def plotting(sample):\n    plt.figure(figsize=(16,6), dpi=100)\n    plt.subplot(211)\n    plt.plot(sample.drop(columns=['onset','wakeup']), 'navy', linewidth=0.5)\n    plt.xlabel('enmo/anglez')\n    \n    plt.subplot(212)\n    plt.plot(sample.onset, 'r--')\n    plt.plot(sample.wakeup, 'b--')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-10T10:57:17.303542Z","iopub.execute_input":"2023-11-10T10:57:17.303954Z","iopub.status.idle":"2023-11-10T10:57:17.311329Z","shell.execute_reply.started":"2023-11-10T10:57:17.303923Z","shell.execute_reply":"2023-11-10T10:57:17.310013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enmo_len_ck_path = '/kaggle/working/train_data/enmo/fine/enmo_id4_128.csv'\nsample = pd.read_csv(enmo_len_ck_path)\nprint(len(sample))\n\nplotting(sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T10:57:43.438334Z","iopub.execute_input":"2023-11-10T10:57:43.438734Z","iopub.status.idle":"2023-11-10T10:57:43.958424Z","shell.execute_reply.started":"2023-11-10T10:57:43.438705Z","shell.execute_reply":"2023-11-10T10:57:43.957268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# enmo_fine1='/kaggle/working/train_data/enmo/fine/enmo_id3_99.csv'\n# enmo_fine2='/kaggle/working/train_data/enmo/fine/enmo_id4_121.csv'\n# enmo_fine3='/kaggle/working/train_data/enmo/fine/enmo_id6_193.csv'\n\n# enmo_hard1='/kaggle/working/train_data/enmo/hard/enmo_id3_120.csv'\n# enmo_hard2='/kaggle/working/train_data/enmo/hard/enmo_id1_45.csv'\n# enmo_hard3='/kaggle/working/train_data/enmo/hard/enmo_id5_158.csv'\n\n# af1='/kaggle/working/train_data/anglez/fine/enmo_id16_450.csv'\n# af2='/kaggle/working/train_data/anglez/fine/enmo_id10_273.csv'\n# af3='/kaggle/working/train_data/anglez/fine/enmo_id6_182.csv'\n\n# ah1='/kaggle/working/train_data/anglez/hard/enmo_id6_197.csv'\n# ah2='/kaggle/working/train_data/anglez/hard/enmo_id4_141.csv'\n# ah3='/kaggle/working/train_data/anglez/hard/enmo_id1_45.csv'","metadata":{"execution":{"iopub.status.busy":"2023-11-10T06:58:09.644779Z","iopub.execute_input":"2023-11-10T06:58:09.645430Z","iopub.status.idle":"2023-11-10T06:58:09.653759Z","shell.execute_reply.started":"2023-11-10T06:58:09.645241Z","shell.execute_reply":"2023-11-10T06:58:09.652340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample = pd.read_csv(enmo_fine1)\n# plotting(sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T06:59:09.638621Z","iopub.execute_input":"2023-11-10T06:59:09.639069Z","iopub.status.idle":"2023-11-10T06:59:10.155142Z","shell.execute_reply.started":"2023-11-10T06:59:09.639037Z","shell.execute_reply":"2023-11-10T06:59:10.153644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample = pd.read_csv(enmo_fine2)\n# plotting(sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T06:59:45.907004Z","iopub.execute_input":"2023-11-10T06:59:45.908575Z","iopub.status.idle":"2023-11-10T06:59:46.397751Z","shell.execute_reply.started":"2023-11-10T06:59:45.908517Z","shell.execute_reply":"2023-11-10T06:59:46.396775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample = pd.read_csv(enmo_fine3)\n# plotting(sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T06:59:46.399548Z","iopub.execute_input":"2023-11-10T06:59:46.399939Z","iopub.status.idle":"2023-11-10T06:59:46.941094Z","shell.execute_reply.started":"2023-11-10T06:59:46.399905Z","shell.execute_reply":"2023-11-10T06:59:46.939862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample = pd.read_csv(af1)\n# plotting(sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T07:00:38.462771Z","iopub.execute_input":"2023-11-10T07:00:38.463273Z","iopub.status.idle":"2023-11-10T07:00:38.985072Z","shell.execute_reply.started":"2023-11-10T07:00:38.463236Z","shell.execute_reply":"2023-11-10T07:00:38.983782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample = pd.read_csv(af2)\n# plotting(sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T07:00:42.046636Z","iopub.execute_input":"2023-11-10T07:00:42.047086Z","iopub.status.idle":"2023-11-10T07:00:42.647880Z","shell.execute_reply.started":"2023-11-10T07:00:42.047050Z","shell.execute_reply":"2023-11-10T07:00:42.646673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample = pd.read_csv(af3)\n# plotting(sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T07:00:42.650330Z","iopub.execute_input":"2023-11-10T07:00:42.650779Z","iopub.status.idle":"2023-11-10T07:00:43.253824Z","shell.execute_reply.started":"2023-11-10T07:00:42.650741Z","shell.execute_reply":"2023-11-10T07:00:43.252531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"hard","metadata":{}},{"cell_type":"code","source":"# sample = pd.read_csv(enmo_hard1)\n# plotting(sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T07:01:01.364993Z","iopub.execute_input":"2023-11-10T07:01:01.365467Z","iopub.status.idle":"2023-11-10T07:01:01.924880Z","shell.execute_reply.started":"2023-11-10T07:01:01.365433Z","shell.execute_reply":"2023-11-10T07:01:01.923549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample = pd.read_csv(enmo_hard2)\n# plotting(sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T07:01:07.116484Z","iopub.execute_input":"2023-11-10T07:01:07.116942Z","iopub.status.idle":"2023-11-10T07:01:07.666320Z","shell.execute_reply.started":"2023-11-10T07:01:07.116907Z","shell.execute_reply":"2023-11-10T07:01:07.665061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample = pd.read_csv(enmo_hard3)\n# plotting(sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T07:01:11.519719Z","iopub.execute_input":"2023-11-10T07:01:11.520230Z","iopub.status.idle":"2023-11-10T07:01:12.097507Z","shell.execute_reply.started":"2023-11-10T07:01:11.520194Z","shell.execute_reply":"2023-11-10T07:01:12.096061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample = pd.read_csv(ah1)\n# plotting(sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T07:01:18.575325Z","iopub.execute_input":"2023-11-10T07:01:18.576595Z","iopub.status.idle":"2023-11-10T07:01:19.082194Z","shell.execute_reply.started":"2023-11-10T07:01:18.576552Z","shell.execute_reply":"2023-11-10T07:01:19.080849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample = pd.read_csv(ah2)\n# plotting(sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T07:01:24.647211Z","iopub.execute_input":"2023-11-10T07:01:24.647659Z","iopub.status.idle":"2023-11-10T07:01:25.231317Z","shell.execute_reply.started":"2023-11-10T07:01:24.647628Z","shell.execute_reply":"2023-11-10T07:01:25.228393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample = pd.read_csv(ah3)\n# plotting(sample)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T07:01:29.517260Z","iopub.execute_input":"2023-11-10T07:01:29.517700Z","iopub.status.idle":"2023-11-10T07:01:30.079337Z","shell.execute_reply.started":"2023-11-10T07:01:29.517668Z","shell.execute_reply":"2023-11-10T07:01:30.077899Z"},"trusted":true},"execution_count":null,"outputs":[]}]}