{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":53666,"databundleVersionId":6589269,"sourceType":"competition"}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Same as sample dataset,\n\nbut this return not truncated data.\n\njust for fast validation.\n\ninference need to do all of these in that notebook.","metadata":{}},{"cell_type":"code","source":"N_DAYS_PER_SAMPLE = 1\nFIT_INTO_1DAY = True\nSEPERATE_FEATURES = True  # Not Used\n# GENERATE_FEATURES = True # generate in Train torch.data.DataSet for efficiency\n\nSIGMA = 720 # target gaussian parameter, 12 * 60","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-17T14:43:20.859769Z","iopub.execute_input":"2023-11-17T14:43:20.860174Z","iopub.status.idle":"2023-11-17T14:43:20.900292Z","shell.execute_reply.started":"2023-11-17T14:43:20.860144Z","shell.execute_reply":"2023-11-17T14:43:20.899240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nimport time\nimport json\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport os\nfrom os.path import join, exists\nimport joblib\nimport random\nimport math\nfrom tqdm.auto import tqdm \nimport shutil\n\nfrom scipy.interpolate import interp1d\n\nfrom math import pi, sqrt, exp\nimport sklearn,sklearn.model_selection\nimport torch\nfrom torch import nn,Tensor\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\nfrom sklearn.metrics import average_precision_score\nfrom timm.scheduler import CosineLRScheduler\nplt.style.use(\"ggplot\")\n\nfrom pyarrow.parquet import ParquetFile\nimport pyarrow as pa \nimport ctypes","metadata":{"execution":{"iopub.status.busy":"2023-11-17T14:43:20.902473Z","iopub.execute_input":"2023-11-17T14:43:20.902932Z","iopub.status.idle":"2023-11-17T14:43:29.645093Z","shell.execute_reply.started":"2023-11-17T14:43:20.902867Z","shell.execute_reply":"2023-11-17T14:43:29.643991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_dir = \"train_data\"\nos.makedirs(out_dir, exist_ok = True)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T14:43:29.646316Z","iopub.execute_input":"2023-11-17T14:43:29.646642Z","iopub.status.idle":"2023-11-17T14:43:29.652774Z","shell.execute_reply.started":"2023-11-17T14:43:29.646615Z","shell.execute_reply":"2023-11-17T14:43:29.651501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PATHS:\n    MAIN_DIR = \"/kaggle/input/child-mind-institute-detect-sleep-states/\"\n    # CSV FILES : \n    TRAIN_EVENTS = MAIN_DIR + \"train_events.csv\"\n    # PARQUET FILES:\n    TRAIN_SERIES = MAIN_DIR + \"train_series.parquet\"\n    SAVE_DIR = out_dir","metadata":{"execution":{"iopub.status.busy":"2023-11-17T14:43:29.654902Z","iopub.execute_input":"2023-11-17T14:43:29.655295Z","iopub.status.idle":"2023-11-17T14:43:29.665895Z","shell.execute_reply.started":"2023-11-17T14:43:29.655265Z","shell.execute_reply":"2023-11-17T14:43:29.664472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_events(data_path, save_dir):\n    \"\"\" events.csv \"\"\"\n    events = pd.read_csv(data_path)\n    if not exists(join(save_dir, 'id_map.parquet')):\n        id_map = pd.DataFrame({\"series_id\": events.series_id.unique(),\n                                \"id_index\": [i for i in range(len(events.series_id.unique()))]})\n        id_map.id_index = id_map.id_index.astype(np.uint16)\n        id_map.to_parquet(os.path.join(save_dir,\"id_map.parquet\"), index=False)\n    id_map = pd.read_parquet(os.path.join(save_dir,\"id_map.parquet\"))\n    events.dropna(subset=['step'], inplace=True)\n    # reduce memory size + event re-label\n    events = events.merge(right=id_map, on=\"series_id\").drop(columns=\"series_id\")\n    events.step = events.step.astype(np.uint32)\n    events.night = events.night.astype(np.uint16)\n#     events.to_csv(os.path.join(save_dir,'preprocessed_events.csv'), index=False)\n    \n    return events","metadata":{"execution":{"iopub.status.busy":"2023-11-17T14:43:36.067235Z","iopub.execute_input":"2023-11-17T14:43:36.067626Z","iopub.status.idle":"2023-11-17T14:43:36.077217Z","shell.execute_reply.started":"2023-11-17T14:43:36.067597Z","shell.execute_reply":"2023-11-17T14:43:36.076004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_series(data_path, save_dir):\n    \"\"\" train_series.parquet \"\"\"\n    # reduce memory\n    series = pd.read_parquet(data_path)\n    if not exists(join(save_dir, 'id_map.parquet')):\n        id_map = pd.DataFrame({\"series_id\": series.series_id.unique(),\n                                \"id_index\": [i for i in range(len(series.series_id.unique()))]})\n        id_map.id_index = id_map.id_index.astype(np.uint16)\n        id_map.to_parquet(os.path.join(save_dir,\"id_map.parquet\"), index=False)\n    \n    id_map = pd.read_parquet(os.path.join(save_dir,\"id_map.parquet\"))\n    series = series.merge(right=id_map, on='series_id').drop(columns='series_id').reset_index()\n    series.step = series.step.astype(np.uint32)\n\n    # normalize enmo and anglez\n    mean_enmo = series['enmo'].mean()\n    std_enmo = series['enmo'].std()\n    series.enmo = (series['enmo'] - mean_enmo)/std_enmo\n    mean_anglez = series['anglez'].mean()\n    std_anglez = series['anglez'].std()\n    series.anglez = (series['anglez'] - mean_anglez)/std_anglez\n    \n    series = series.drop(columns='index')\n#     series.to_parquet(os.path.join(save_dir,'preprocessed_series.parquet'))\n    \n    return series","metadata":{"execution":{"iopub.status.busy":"2023-11-17T14:43:36.193876Z","iopub.execute_input":"2023-11-17T14:43:36.194346Z","iopub.status.idle":"2023-11-17T14:43:36.205642Z","shell.execute_reply.started":"2023-11-17T14:43:36.194314Z","shell.execute_reply":"2023-11-17T14:43:36.204295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"events = preprocess_events(PATHS.TRAIN_EVENTS, PATHS.SAVE_DIR)\nseries = preprocess_series(PATHS.TRAIN_SERIES, PATHS.SAVE_DIR).drop(columns=['timestamp','step'])\nids = pd.read_parquet(os.path.join(PATHS.SAVE_DIR,\"id_map.parquet\"))","metadata":{"execution":{"iopub.status.busy":"2023-11-17T14:43:36.740619Z","iopub.execute_input":"2023-11-17T14:43:36.741157Z","iopub.status.idle":"2023-11-17T14:46:01.991249Z","shell.execute_reply.started":"2023-11-17T14:43:36.741121Z","shell.execute_reply":"2023-11-17T14:46:01.989876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# store series with target gaussian\n\n- raw form with preprocessed, target gaussian","metadata":{}},{"cell_type":"code","source":"def gauss(n=SIGMA,sigma=SIGMA*0.15):\n    # guassian distribution function\n    r = range(-int(n/2),int(n/2)+1)  # int는 나머지 버림계산\n    return [1 / (sigma * sqrt(2*pi)) * exp(-float(x)**2/(2*sigma**2)) for x in r]\n\n\nfor cur_id in tqdm(ids.id_index, total=len(ids)):\n    cur_events = events[events.id_index == cur_id].copy()\n    cur_series = cur_series=series.loc[series.id_index==cur_id].copy().reset_index(drop=True).drop(columns='id_index')\n    \n    # make gaussian\n    target_gaussian = np.zeros((len(cur_series),2))\n    for i in range(len(cur_events)):\n        s = cur_events.iloc[i].step\n        tar_st=s-SIGMA//2\n        tar_ed=s+SIGMA//2\n        if tar_st<0:\n            gau_st = -(SIGMA//2-s)\n            tar_st = 0\n        else:\n            gau_st=0\n        if tar_ed>=len(cur_series):\n            residual = tar_ed-(len(cur_series)-1)\n            gau_ed=SIGMA-residual\n            tar_ed=len(cur_series)-1\n        else:\n            gau_ed=SIGMA\n        \n#         if len(target_gaussian[tar_st:tar_ed,0])!=len(gauss()[gau_st:gau_ed]):\n#             print(tar_st, tar_ed)\n#             print(gau_st, gau_ed)\n#             print(len(target_gaussian[tar_st:tar_ed,0]))\n#             print(len(gauss()[gau_st:gau_ed]))\n        if cur_events.iloc[i].event=='onset':\n            target_gaussian[tar_st:tar_ed,0] = gauss()[gau_st:gau_ed]\n        if cur_events.iloc[i].event=='wakeup':\n            target_gaussian[tar_st:tar_ed,1] = gauss()[gau_st:gau_ed]\n    # normalize target into max value.\n    target_gaussian /= np.max(target_gaussian + 1e-12)\n    # append target onto series,\n    cur_series['onset'] = target_gaussian[:,0]\n    cur_series['wakeup'] = target_gaussian[:,1]\n    # save it.\n    cur_full_id=ids.loc[ids.id_index==cur_id].series_id.item()\n    cur_series.to_csv(f'{PATHS.SAVE_DIR}/{cur_full_id}_{cur_id}.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T14:46:01.993607Z","iopub.execute_input":"2023-11-17T14:46:01.994052Z","iopub.status.idle":"2023-11-17T14:59:44.546314Z","shell.execute_reply.started":"2023-11-17T14:46:01.994018Z","shell.execute_reply":"2023-11-17T14:59:44.544748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = pd.read_csv('/kaggle/working/train_data/8b8b9e29171c_144.csv')\ns[:3]","metadata":{"execution":{"iopub.status.busy":"2023-11-17T15:09:37.312204Z","iopub.execute_input":"2023-11-17T15:09:37.312610Z","iopub.status.idle":"2023-11-17T15:09:37.673735Z","shell.execute_reply.started":"2023-11-17T15:09:37.312580Z","shell.execute_reply":"2023-11-17T15:09:37.672883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e = events.loc[events.id_index==144]\ne[:4]","metadata":{"execution":{"iopub.status.busy":"2023-11-17T15:09:42.522832Z","iopub.execute_input":"2023-11-17T15:09:42.523306Z","iopub.status.idle":"2023-11-17T15:09:42.541133Z","shell.execute_reply.started":"2023-11-17T15:09:42.523266Z","shell.execute_reply":"2023-11-17T15:09:42.539826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = s[['onset','wakeup']].to_numpy()\nt = np.zeros_like(y)\nfor i in range(len(e)):\n    if e.iloc[i].event=='onset':\n        t[e.iloc[i].step,0] = 1\n    elif e.iloc[i].event=='wakeup':\n        t[e.iloc[i].step,1] = 1","metadata":{"execution":{"iopub.status.busy":"2023-11-17T15:18:45.678188Z","iopub.execute_input":"2023-11-17T15:18:45.678603Z","iopub.status.idle":"2023-11-17T15:18:45.705794Z","shell.execute_reply.started":"2023-11-17T15:18:45.678573Z","shell.execute_reply":"2023-11-17T15:18:45.704780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.plot(y.T[0][:200000], 'navy', t.T[0][:200000], 'r--', linewidth=0.8)\nplt.ylim(bottom=0, top=1)\nplt.xlabel('pred - onset')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# save","metadata":{}},{"cell_type":"code","source":"os.makedirs('./archive', exist_ok=True)\nshutil.make_archive(f'./archive/data', 'zip', f'{PATHS.SAVE_DIR}')\nshutil.move(f'./{PATHS.SAVE_DIR}/id_map.parquet', './archive')\nshutil.rmtree(PATHS.SAVE_DIR)\n\n\n# Creating a dictionary with dummy data for testing\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emma'],\n    'Age': [25, 30, 35, 40, 45],\n    'City': ['New York', 'San Francisco', 'Los Angeles', 'Chicago', 'Seattle']\n}\n\na = pd.DataFrame(data)\na.to_csv('./archive/dummy.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T09:27:55.585667Z","iopub.execute_input":"2023-11-16T09:27:55.586186Z","iopub.status.idle":"2023-11-16T09:29:51.613802Z","shell.execute_reply.started":"2023-11-16T09:27:55.586146Z","shell.execute_reply":"2023-11-16T09:29:51.611761Z"},"trusted":true},"execution_count":null,"outputs":[]}]}